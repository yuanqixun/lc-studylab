"""
åŸºç¡€ Agent æ¨¡å—
ä½¿ç”¨ LangChain 1.0.3 çš„å…¨æ–° create_agent API å®ç°é€šç”¨çš„æ™ºèƒ½ä½“å°è£…

è¿™æ˜¯ç¬¬ 1 é˜¶æ®µçš„æ ¸å¿ƒæ¨¡å—ï¼Œå®ç°ï¼š
1. åŸºäº LangChain V1.0.0 çš„ create_agent API
2. æµå¼è¾“å‡ºæ”¯æŒï¼ˆStreamingï¼‰
3. å·¥å…·è°ƒç”¨é›†æˆ
4. ç»Ÿä¸€çš„æ¶ˆæ¯å¤„ç†

æŠ€æœ¯è¦ç‚¹ï¼š
- ä½¿ç”¨ LangChain 1.0.3 çš„ langchain.agents.create_agent API
- create_agent è¿”å› CompiledStateGraphï¼ˆåŸºäº LangGraphï¼‰
- æ”¯æŒæµå¼è¾“å‡ºï¼Œå¯ä»¥å®æ—¶çœ‹åˆ° tokenã€tool callsã€reasoning
- é›†æˆè‡ªå®šä¹‰å·¥å…·ï¼ˆæ—¶é—´ã€è®¡ç®—ã€æœç´¢ç­‰ï¼‰
- æä¾›åŒæ­¥å’Œå¼‚æ­¥æ¥å£

å‚è€ƒæ–‡æ¡£ï¼š
- https://docs.langchain.com/oss/python/langchain/agents
- https://reference.langchain.com/python/langchain/agents/
"""

from typing import List, Optional, Dict, Any, Iterator, AsyncIterator, Union, Sequence
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_core.language_models.chat_models import BaseChatModel
from langchain.agents import create_agent  # LangChain V1.0.0 çš„æ–° API

from core.models import get_chat_model, get_streaming_model
from core.prompts import get_system_prompt, get_prompt_with_tools
from core.tools import ALL_TOOLS, BASIC_TOOLS
from config import settings, get_logger

logger = get_logger(__name__)


class BaseAgent:
    """
    åŸºç¡€ Agent ç±»
    
    å°è£…äº† LangChain 1.0.3 çš„ create_agent åŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„æ™ºèƒ½ä½“æ¥å£ã€‚
    
    åœ¨ LangChain V1.0.0 ä¸­ï¼Œcreate_agent è¿”å›ä¸€ä¸ª CompiledStateGraphï¼ˆåŸºäº LangGraphï¼‰ï¼Œ
    å®ƒå†…éƒ¨å·²ç»å®ç°äº†å®Œæ•´çš„å·¥å…·è°ƒç”¨å¾ªç¯ã€çŠ¶æ€ç®¡ç†å’Œæµå¼è¾“å‡ºã€‚
    
    Attributes:
        model: LLM æ¨¡å‹å®ä¾‹æˆ–æ¨¡å‹æ ‡è¯†ç¬¦
        tools: Agent å¯ç”¨çš„å·¥å…·åˆ—è¡¨
        graph: LangChain çš„ CompiledStateGraph å®ä¾‹ï¼ˆç”± create_agent è¿”å›ï¼‰
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        
    Example:
        >>> # åˆ›å»ºä¸€ä¸ªåŸºç¡€ Agent
        >>> agent = BaseAgent(tools=[get_current_time, calculator])
        >>> 
        >>> # åŒæ­¥è°ƒç”¨
        >>> response = agent.invoke("ç°åœ¨å‡ ç‚¹ï¼Ÿ")
        >>> print(response)
        >>> 
        >>> # æµå¼è°ƒç”¨
        >>> for chunk in agent.stream("è®¡ç®— 123 + 456"):
        ...     print(chunk, end="", flush=True)
    
    å‚è€ƒï¼š
        - https://docs.langchain.com/oss/python/langchain/agents
        - https://reference.langchain.com/python/langchain/agents/
    """
    
    def __init__(
        self,
        model: Optional[Union[str, BaseChatModel]] = None,
        tools: Optional[Sequence[BaseTool]] = None,
        system_prompt: Optional[str] = None,
        prompt_mode: str = "default",
        debug: bool = False,
        **kwargs: Any,
    ):
        """
        åˆå§‹åŒ– Base Agent
        
        æ ¹æ® LangChain V1.0.0 çš„ create_agent API è§„èŒƒåˆå§‹åŒ– Agentã€‚
        
        Args:
            model: LLM æ¨¡å‹ï¼Œå¯ä»¥æ˜¯ï¼š
                   - å­—ç¬¦ä¸²æ ‡è¯†ç¬¦ï¼ˆå¦‚ "openai:gpt-4o"ï¼‰
                   - BaseChatModel å®ä¾‹
                   å¦‚æœä¸º Noneï¼Œä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»º
            tools: Agent å¯ç”¨çš„å·¥å…·åˆ—è¡¨ï¼ˆSequence[BaseTool]ï¼‰
                   å¦‚æœä¸º None æˆ–ç©ºåˆ—è¡¨ï¼ŒAgent å°†åªåŒ…å«æ¨¡å‹èŠ‚ç‚¹ï¼Œä¸è¿›è¡Œå·¥å…·è°ƒç”¨å¾ªç¯
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
                          å¦‚æœä¸º Noneï¼Œåˆ™æ ¹æ® prompt_mode ç”Ÿæˆ
            prompt_mode: æç¤ºè¯æ¨¡å¼ï¼ˆdefault/coding/research/concise/detailedï¼‰
            debug: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼ˆå¯¹åº” create_agent çš„ debug å‚æ•°ï¼‰
            **kwargs: å…¶ä»–ä¼ é€’ç»™ create_agent çš„å‚æ•°ï¼Œå¦‚ï¼š
                     - checkpointer: çŠ¶æ€æŒä¹…åŒ–
                     - store: è·¨çº¿ç¨‹æ•°æ®å­˜å‚¨
                     - interrupt_before/interrupt_after: ä¸­æ–­ç‚¹
                     - name: Agent åç§°
        
        å‚è€ƒï¼š
            https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent
        """
        # ==================== æ¨¡å‹åˆå§‹åŒ– ====================
        # åœ¨ LangChain V1.0.0 ä¸­ï¼Œmodel å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ– BaseChatModel å®ä¾‹
        if model is None:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼ˆä»é…ç½®è¯»å–ï¼‰
            # create_agent æ¥å—å­—ç¬¦ä¸²æ ¼å¼ï¼Œå¦‚ "openai:gpt-4o"
            self.model = f"openai:{settings.openai_model}"
            logger.info(f"ğŸ¤– ä½¿ç”¨é»˜è®¤æ¨¡å‹: {self.model}")
        elif isinstance(model, str):
            # å­—ç¬¦ä¸²æ ‡è¯†ç¬¦
            self.model = model
            logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹æ ‡è¯†ç¬¦: {model}")
        else:
            # BaseChatModel å®ä¾‹
            self.model = model
            logger.info(f"ğŸ¤– ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹å®ä¾‹: {model.__class__.__name__}")
        
        # ==================== å·¥å…·åˆå§‹åŒ– ====================
        if tools is None:
            # é»˜è®¤ä½¿ç”¨åŸºç¡€å·¥å…·é›†ï¼ˆä¸éœ€è¦ API Keyï¼‰
            self.tools = BASIC_TOOLS
            logger.info(f"ğŸ”§ ä½¿ç”¨åŸºç¡€å·¥å…·é›† ({len(self.tools)} ä¸ªå·¥å…·)")
        else:
            self.tools = list(tools) if tools else []
            logger.info(f"ğŸ”§ ä½¿ç”¨è‡ªå®šä¹‰å·¥å…·é›† ({len(self.tools)} ä¸ªå·¥å…·)")
        
        # æ‰“å°å·¥å…·åˆ—è¡¨
        if self.tools:
            tool_names = [tool.name for tool in self.tools]
            logger.debug(f"   å·¥å…·åˆ—è¡¨: {', '.join(tool_names)}")
        
        # ==================== æç¤ºè¯åˆå§‹åŒ– ====================
        if system_prompt is None:
            # æ ¹æ®æ¨¡å¼ç”Ÿæˆç³»ç»Ÿæç¤ºè¯
            if self.tools:
                # å¦‚æœæœ‰å·¥å…·ï¼Œä½¿ç”¨åŒ…å«å·¥å…·è¯´æ˜çš„æç¤ºè¯
                self.system_prompt = get_prompt_with_tools(mode=prompt_mode)
                logger.info(f"ğŸ“ ä½¿ç”¨å¸¦å·¥å…·è¯´æ˜çš„ç³»ç»Ÿæç¤ºè¯ (æ¨¡å¼: {prompt_mode})")
            else:
                # æ²¡æœ‰å·¥å…·ï¼Œä½¿ç”¨æ™®é€šæç¤ºè¯
                self.system_prompt = get_system_prompt(mode=prompt_mode)
                logger.info(f"ğŸ“ ä½¿ç”¨æ™®é€šç³»ç»Ÿæç¤ºè¯ (æ¨¡å¼: {prompt_mode})")
        else:
            self.system_prompt = system_prompt
            logger.info("ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯")
        
        # ==================== Agent é…ç½® ====================
        self.debug = debug
        
        # ==================== åˆ›å»º Agent ====================
        # åœ¨ LangChain V1.0.0 ä¸­ï¼Œä½¿ç”¨ create_agent ç›´æ¥åˆ›å»º
        # å®ƒè¿”å›ä¸€ä¸ª CompiledStateGraphï¼Œå†…éƒ¨å·²ç»å®ç°äº†å®Œæ•´çš„å·¥å…·è°ƒç”¨å¾ªç¯
        try:
            logger.info("ğŸ”¨ åˆ›å»º Agentï¼ˆä½¿ç”¨ LangChain V1.0.0 create_agent APIï¼‰...")
            
            # è°ƒç”¨ create_agent
            # å‚è€ƒï¼šhttps://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent
            self.graph = create_agent(
                model=self.model,
                tools=self.tools if self.tools else None,  # None æˆ–ç©ºåˆ—è¡¨è¡¨ç¤ºæ— å·¥å…·
                system_prompt=self.system_prompt,
                debug=self.debug,
                **kwargs,  # æ”¯æŒ checkpointer, store, interrupt_before/after, name ç­‰
            )
            
            logger.info("âœ… Agent åˆ›å»ºæˆåŠŸï¼ˆCompiledStateGraphï¼‰")
            logger.debug(f"   é…ç½®: debug={self.debug}, tools={len(self.tools)}")
            
        except Exception as e:
            logger.error(f"âŒ Agent åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def invoke(
        self,
        input_text: str,
        chat_history: Optional[List[BaseMessage]] = None,
        **kwargs: Any,
    ) -> str:
        """
        åŒæ­¥è°ƒç”¨ Agentï¼ˆéæµå¼ï¼‰
        
        åœ¨ LangChain V1.0.0 ä¸­ï¼Œcreate_agent è¿”å›çš„ CompiledStateGraph
        ä½¿ç”¨ {"messages": [...]} ä½œä¸ºè¾“å…¥æ ¼å¼ã€‚
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            chat_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–ä¼ é€’ç»™ graph çš„å‚æ•°
            
        Returns:
            Agent çš„å“åº”æ–‡æœ¬
            
        Example:
            >>> agent = BaseAgent()
            >>> response = agent.invoke("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
            >>> print(response)
        
        å‚è€ƒï¼š
            https://docs.langchain.com/oss/python/langchain/agents
        """
        logger.info(f"ğŸš€ æ‰§è¡Œ Agent è°ƒç”¨: {input_text[:50]}...")
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            # LangChain V1.0.0 çš„ create_agent ä½¿ç”¨ {"messages": [...]} æ ¼å¼
            messages = []
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            if chat_history:
                messages.extend(chat_history)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append(HumanMessage(content=input_text))
            
            # å‡†å¤‡è¾“å…¥
            graph_input = {"messages": messages}
            graph_input.update(kwargs)
            
            # æ‰§è¡Œ Graph
            # CompiledStateGraph çš„ invoke æ–¹æ³•è¿”å›æœ€ç»ˆçŠ¶æ€
            result = self.graph.invoke(graph_input)
            
            # æå–æœ€åä¸€æ¡ AI æ¶ˆæ¯
            # result æ˜¯ä¸€ä¸ªåŒ…å« "messages" é”®çš„å­—å…¸
            output_messages = result.get("messages", [])
            
            # æ‰¾åˆ°æœ€åä¸€æ¡ AI æ¶ˆæ¯
            ai_response = ""
            for msg in reversed(output_messages):
                if isinstance(msg, AIMessage):
                    ai_response = msg.content
                    break
            
            logger.info(f"âœ… Agent è°ƒç”¨å®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(ai_response)} å­—ç¬¦")
            logger.debug(f"   è¾“å‡º: {ai_response[:100]}...")
            
            return ai_response
            
        except Exception as e:
            error_msg = f"Agent æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def stream(
        self,
        input_text: str,
        chat_history: Optional[List[BaseMessage]] = None,
        stream_mode: str = "messages",
        **kwargs: Any,
    ) -> Iterator[str]:
        """
        æµå¼è°ƒç”¨ Agent
        
        åœ¨ LangChain V1.0.0 ä¸­ï¼ŒCompiledStateGraph æ”¯æŒå¤šç§æµå¼æ¨¡å¼ã€‚
        é»˜è®¤ä½¿ç”¨ "messages" æ¨¡å¼ï¼Œé€æ­¥è¿”å›æ¶ˆæ¯å†…å®¹ã€‚
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            chat_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            stream_mode: æµå¼æ¨¡å¼ï¼Œå¯é€‰å€¼ï¼š
                        - "messages": æµå¼è¿”å›æ¶ˆæ¯å†…å®¹ï¼ˆæ¨èï¼‰
                        - "updates": è¿”å›çŠ¶æ€æ›´æ–°
                        - "values": è¿”å›å®Œæ•´çŠ¶æ€å€¼
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            Agent è¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
            
        Example:
            >>> agent = BaseAgent()
            >>> for chunk in agent.stream("è®²ä¸ªç¬‘è¯"):
            ...     print(chunk, end="", flush=True)
        
        å‚è€ƒï¼š
            https://docs.langchain.com/oss/python/langchain/agents
        """
        logger.info(f"ğŸŒŠ æ‰§è¡Œ Agent æµå¼è°ƒç”¨: {input_text[:50]}...")
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = []
            if chat_history:
                messages.extend(chat_history)
            messages.append(HumanMessage(content=input_text))
            
            # å‡†å¤‡è¾“å…¥
            graph_input = {"messages": messages}
            graph_input.update(kwargs)
            
            # æµå¼æ‰§è¡Œ Graph
            # CompiledStateGraph çš„ stream æ–¹æ³•æ”¯æŒå¤šç§æ¨¡å¼
            for chunk in self.graph.stream(graph_input, stream_mode=stream_mode):
                # æ ¹æ® stream_mode å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
                if stream_mode == "messages":
                    # messages æ¨¡å¼ï¼šchunk æ˜¯ (message, metadata) å…ƒç»„
                    if isinstance(chunk, tuple) and len(chunk) == 2:
                        message, metadata = chunk
                        if isinstance(message, AIMessage) and message.content:
                            logger.debug(f"   æµå¼è¾“å‡º: {message.content[:50]}...")
                            yield message.content
                    elif isinstance(chunk, AIMessage) and chunk.content:
                        logger.debug(f"   æµå¼è¾“å‡º: {chunk.content[:50]}...")
                        yield chunk.content
                
                elif stream_mode == "updates":
                    # updates æ¨¡å¼ï¼šchunk æ˜¯çŠ¶æ€æ›´æ–°å­—å…¸
                    if isinstance(chunk, dict) and "messages" in chunk:
                        messages_update = chunk["messages"]
                        if messages_update:
                            last_msg = messages_update[-1]
                            if isinstance(last_msg, AIMessage) and last_msg.content:
                                yield last_msg.content
            
            logger.info("âœ… Agent æµå¼è°ƒç”¨å®Œæˆ")
            
        except Exception as e:
            error_msg = f"Agent æµå¼æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            yield f"\n\næŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    async def ainvoke(
        self,
        input_text: str,
        chat_history: Optional[List[BaseMessage]] = None,
        **kwargs: Any,
    ) -> str:
        """
        å¼‚æ­¥è°ƒç”¨ Agentï¼ˆéæµå¼ï¼‰
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            chat_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            Agent çš„å“åº”æ–‡æœ¬
            
        Example:
            >>> agent = BaseAgent()
            >>> response = await agent.ainvoke("ä½ å¥½")
            >>> print(response)
        """
        logger.info(f"ğŸš€ æ‰§è¡Œ Agent å¼‚æ­¥è°ƒç”¨: {input_text[:50]}...")
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = []
            if chat_history:
                messages.extend(chat_history)
            messages.append(HumanMessage(content=input_text))
            
            # å‡†å¤‡è¾“å…¥
            graph_input = {"messages": messages}
            graph_input.update(kwargs)
            
            # å¼‚æ­¥æ‰§è¡Œ Graph
            result = await self.graph.ainvoke(graph_input)
            
            # æå–æœ€åä¸€æ¡ AI æ¶ˆæ¯
            output_messages = result.get("messages", [])
            ai_response = ""
            for msg in reversed(output_messages):
                if isinstance(msg, AIMessage):
                    ai_response = msg.content
                    break
            
            logger.info(f"âœ… Agent å¼‚æ­¥è°ƒç”¨å®Œæˆ")
            return ai_response
            
        except Exception as e:
            error_msg = f"Agent å¼‚æ­¥æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    async def astream(
        self,
        input_text: str,
        chat_history: Optional[List[BaseMessage]] = None,
        stream_mode: str = "messages",
        **kwargs: Any,
    ) -> AsyncIterator[str]:
        """
        å¼‚æ­¥æµå¼è°ƒç”¨ Agent
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            chat_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            stream_mode: æµå¼æ¨¡å¼ï¼ˆ"messages" æˆ– "updates"ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            Agent è¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
            
        Example:
            >>> agent = BaseAgent()
            >>> async for chunk in agent.astream("è®²ä¸ªç¬‘è¯"):
            ...     print(chunk, end="", flush=True)
        """
        logger.info(f"ğŸŒŠ æ‰§è¡Œ Agent å¼‚æ­¥æµå¼è°ƒç”¨: {input_text[:50]}...")
        
        try:
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = []
            if chat_history:
                messages.extend(chat_history)
            messages.append(HumanMessage(content=input_text))
            
            # å‡†å¤‡è¾“å…¥
            graph_input = {"messages": messages}
            graph_input.update(kwargs)
            
            # å¼‚æ­¥æµå¼æ‰§è¡Œ Graph
            async for chunk in self.graph.astream(graph_input, stream_mode=stream_mode):
                # æ ¹æ® stream_mode å¤„ç†ä¸åŒçš„è¾“å‡ºæ ¼å¼
                if stream_mode == "messages":
                    if isinstance(chunk, tuple) and len(chunk) == 2:
                        message, metadata = chunk
                        if isinstance(message, AIMessage) and message.content:
                            yield message.content
                    elif isinstance(chunk, AIMessage) and chunk.content:
                        yield chunk.content
                
                elif stream_mode == "updates":
                    if isinstance(chunk, dict) and "messages" in chunk:
                        messages_update = chunk["messages"]
                        if messages_update:
                            last_msg = messages_update[-1]
                            if isinstance(last_msg, AIMessage) and last_msg.content:
                                yield last_msg.content
            
            logger.info("âœ… Agent å¼‚æ­¥æµå¼è°ƒç”¨å®Œæˆ")
            
        except Exception as e:
            error_msg = f"Agent å¼‚æ­¥æµå¼æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            yield f"\n\næŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"


def create_base_agent(
    model: Optional[Union[str, BaseChatModel]] = None,
    tools: Optional[Sequence[BaseTool]] = None,
    prompt_mode: str = "default",
    debug: bool = False,
    **kwargs: Any,
) -> BaseAgent:
    """
    åˆ›å»ºåŸºç¡€ Agent çš„ä¾¿æ·å·¥å‚å‡½æ•°
    
    æ ¹æ® LangChain V1.0.0 çš„è§„èŒƒåˆ›å»º Agentã€‚
    
    Args:
        model: LLM æ¨¡å‹ï¼ˆå­—ç¬¦ä¸²æ ‡è¯†ç¬¦æˆ–å®ä¾‹ï¼‰
        tools: å·¥å…·åˆ—è¡¨
        prompt_mode: æç¤ºè¯æ¨¡å¼
        debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ—¥å¿—
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆä¼ é€’ç»™ create_agentï¼‰
        
    Returns:
        é…ç½®å¥½çš„ BaseAgent å®ä¾‹
        
    Example:
        >>> # åˆ›å»ºé»˜è®¤ Agent
        >>> agent = create_base_agent()
        >>> 
        >>> # åˆ›å»ºç¼–ç¨‹åŠ©æ‰‹ Agent
        >>> agent = create_base_agent(prompt_mode="coding")
        >>> 
        >>> # åˆ›å»ºå¸¦æ‰€æœ‰å·¥å…·çš„ Agent
        >>> from core.tools import ALL_TOOLS
        >>> agent = create_base_agent(tools=ALL_TOOLS)
        >>> 
        >>> # ä½¿ç”¨ç‰¹å®šæ¨¡å‹
        >>> agent = create_base_agent(model="openai:gpt-4o-mini")
    
    å‚è€ƒï¼š
        https://docs.langchain.com/oss/python/langchain/agents
    """
    logger.info(f"ğŸ­ åˆ›å»º Base Agent (mode={prompt_mode}, debug={debug})")
    
    return BaseAgent(
        model=model,
        tools=tools,
        prompt_mode=prompt_mode,
        debug=debug,
        **kwargs,
    )

