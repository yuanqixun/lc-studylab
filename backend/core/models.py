"""
æ¨¡å‹å°è£…æ¨¡å—
æä¾›ç»Ÿä¸€çš„ LLM æ¨¡å‹æ¥å£ï¼Œæ”¯æŒ OpenAI ç­‰å¤šç§æä¾›å•†

ä½¿ç”¨ LangChain 1.0.3 çš„æ ‡å‡†æ¥å£å°è£…æ¨¡å‹

åœ¨ LangChain V1.0.0 ä¸­ï¼Œcreate_agent æ¥å—å­—ç¬¦ä¸²æ ¼å¼çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œ
å¦‚ "openai:gpt-4o"ï¼Œè¿™æ ·å¯ä»¥è‡ªåŠ¨åˆå§‹åŒ–æ¨¡å‹å¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ API Keyã€‚

å‚è€ƒï¼š
- https://docs.langchain.com/oss/python/langchain/models
- https://reference.langchain.com/python/langchain/models/
"""

from typing import Optional, Dict, Any
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel

from config import settings, get_logger

logger = get_logger(__name__)


def get_chat_model(
    model_name: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    streaming: Optional[bool] = None,
    **kwargs: Any,
) -> BaseChatModel:
    """
    è·å–é…ç½®å¥½çš„èŠå¤©æ¨¡å‹å®ä¾‹
    
    è¿™æ˜¯ä¸€ä¸ªå·¥å‚å‡½æ•°ï¼Œæ ¹æ®é…ç½®åˆ›å»º LangChain çš„ ChatModel å®ä¾‹ã€‚
    é»˜è®¤ä½¿ç”¨ OpenAI çš„æ¨¡å‹ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œè‡ªå®šä¹‰å‚æ•°ã€‚
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ openai_model
        temperature: æ¸©åº¦å‚æ•° (0.0-2.0)ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        **kwargs: å…¶ä»–ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„ ChatModel å®ä¾‹
        
    Example:
        >>> # ä½¿ç”¨é»˜è®¤é…ç½®
        >>> model = get_chat_model()
        >>> 
        >>> # è‡ªå®šä¹‰å‚æ•°
        >>> model = get_chat_model(
        ...     model_name="gpt-4o-mini",
        ...     temperature=0.5,
        ...     streaming=True
        ... )
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    model_name = model_name or settings.openai_model
    temperature = temperature if temperature is not None else settings.openai_temperature
    streaming = streaming if streaming is not None else settings.openai_streaming
    
    # æ„å»ºæ¨¡å‹é…ç½®
    model_config: Dict[str, Any] = {
        "model": model_name,
        "temperature": temperature,
        "streaming": streaming,
        "api_key": settings.openai_api_key,
        "base_url": settings.openai_api_base,
    }
    
    # æ·»åŠ å¯é€‰çš„ max_tokens
    if max_tokens is not None:
        model_config["max_tokens"] = max_tokens
    elif settings.openai_max_tokens is not None:
        model_config["max_tokens"] = settings.openai_max_tokens
    
    # åˆå¹¶é¢å¤–çš„å‚æ•°
    model_config.update(kwargs)
    
    logger.info(
        f"ğŸ¤– åˆ›å»ºèŠå¤©æ¨¡å‹: {model_name} "
        f"(temperature={temperature}, streaming={streaming})"
    )
    
    # åˆ›å»º OpenAI ChatModel å®ä¾‹
    # è¿™é‡Œä½¿ç”¨ LangChain 1.0.3 çš„æ ‡å‡†æ¥å£
    try:
        model = ChatOpenAI(**model_config)
        logger.debug(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ: {model_name}")
        return model
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        raise


def get_streaming_model(
    model_name: Optional[str] = None,
    temperature: Optional[float] = None,
    **kwargs: Any,
) -> BaseChatModel:
    """
    è·å–å¯ç”¨æµå¼è¾“å‡ºçš„èŠå¤©æ¨¡å‹
    
    è¿™æ˜¯ get_chat_model çš„ä¾¿æ·åŒ…è£…ï¼Œå¼ºåˆ¶å¯ç”¨æµå¼è¾“å‡ºã€‚
    
    Args:
        model_name: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        å¯ç”¨æµå¼è¾“å‡ºçš„ ChatModel å®ä¾‹
        
    Example:
        >>> model = get_streaming_model()
        >>> for chunk in model.stream("ä½ å¥½"):
        ...     print(chunk.content, end="", flush=True)
    """
    return get_chat_model(
        model_name=model_name,
        temperature=temperature,
        streaming=True,
        **kwargs,
    )


def get_structured_output_model(
    model_name: Optional[str] = None,
    temperature: float = 0.0,
    **kwargs: Any,
) -> BaseChatModel:
    """
    è·å–ç”¨äºç»“æ„åŒ–è¾“å‡ºçš„èŠå¤©æ¨¡å‹
    
    ç»“æ„åŒ–è¾“å‡ºé€šå¸¸éœ€è¦æ›´ä½çš„æ¸©åº¦ä»¥ç¡®ä¿è¾“å‡ºæ ¼å¼çš„ä¸€è‡´æ€§ã€‚
    
    Args:
        model_name: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä¸º 0.0ï¼ˆæ›´ç¡®å®šæ€§çš„è¾“å‡ºï¼‰
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        é…ç½®ä¸ºç»“æ„åŒ–è¾“å‡ºçš„ ChatModel å®ä¾‹
        
    Example:
        >>> from pydantic import BaseModel
        >>> 
        >>> class Answer(BaseModel):
        ...     answer: str
        ...     confidence: float
        >>> 
        >>> model = get_structured_output_model()
        >>> structured_model = model.with_structured_output(Answer)
        >>> result = structured_model.invoke("What is 2+2?")
    """
    return get_chat_model(
        model_name=model_name,
        temperature=temperature,
        streaming=False,  # ç»“æ„åŒ–è¾“å‡ºé€šå¸¸ä¸ä½¿ç”¨æµå¼
        **kwargs,
    )


# é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "default": {
        "model_name": "gpt-4o",
        "temperature": 0.7,
        "description": "é»˜è®¤æ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬",
    },
    "fast": {
        "model_name": "gpt-4o-mini",
        "temperature": 0.7,
        "description": "å¿«é€Ÿæ¨¡å‹ï¼Œé€‚åˆç®€å•ä»»åŠ¡",
    },
    "precise": {
        "model_name": "gpt-4o",
        "temperature": 0.3,
        "description": "ç²¾ç¡®æ¨¡å‹ï¼Œé€‚åˆéœ€è¦å‡†ç¡®æ€§çš„ä»»åŠ¡",
    },
    "creative": {
        "model_name": "gpt-4o",
        "temperature": 1.0,
        "description": "åˆ›æ„æ¨¡å‹ï¼Œé€‚åˆéœ€è¦åˆ›é€ æ€§çš„ä»»åŠ¡",
    },
}


def get_model_by_preset(preset: str = "default", **kwargs: Any) -> BaseChatModel:
    """
    æ ¹æ®é¢„è®¾é…ç½®è·å–æ¨¡å‹
    
    Args:
        preset: é¢„è®¾åç§°ï¼Œå¯é€‰å€¼: default, fast, precise, creative
        **kwargs: è¦†ç›–é¢„è®¾çš„å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„ ChatModel å®ä¾‹
        
    Raises:
        ValueError: å¦‚æœé¢„è®¾åç§°ä¸å­˜åœ¨
        
    Example:
        >>> # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
        >>> model = get_model_by_preset("fast")
        >>> 
        >>> # ä½¿ç”¨ç²¾ç¡®æ¨¡å‹ï¼Œä½†è¦†ç›–æ¸©åº¦
        >>> model = get_model_by_preset("precise", temperature=0.1)
    """
    if preset not in MODEL_CONFIGS:
        available = ", ".join(MODEL_CONFIGS.keys())
        raise ValueError(f"æœªçŸ¥çš„é¢„è®¾: {preset}. å¯ç”¨é¢„è®¾: {available}")
    
    config = MODEL_CONFIGS[preset].copy()
    config.pop("description", None)  # ç§»é™¤æè¿°å­—æ®µ
    config.update(kwargs)  # ç”¨æˆ·å‚æ•°è¦†ç›–é¢„è®¾
    
    logger.info(f"ğŸ“‹ ä½¿ç”¨é¢„è®¾æ¨¡å‹é…ç½®: {preset}")
    return get_chat_model(**config)


def get_model_string(
    model_name: Optional[str] = None,
    provider: str = "openai",
) -> str:
    """
    è·å–æ¨¡å‹æ ‡è¯†ç¬¦å­—ç¬¦ä¸²
    
    åœ¨ LangChain V1.0.0 ä¸­ï¼Œcreate_agent æ¥å—å­—ç¬¦ä¸²æ ¼å¼çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œ
    å¦‚ "openai:gpt-4o"ã€"anthropic:claude-3-5-sonnet-20241022" ç­‰ã€‚
    
    è¿™ä¸ªå‡½æ•°æ ¹æ®é…ç½®ç”Ÿæˆæ­£ç¡®çš„æ¨¡å‹æ ‡è¯†ç¬¦å­—ç¬¦ä¸²ã€‚
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
        provider: æä¾›å•†åç§°ï¼Œé»˜è®¤ä¸º "openai"
        
    Returns:
        æ¨¡å‹æ ‡è¯†ç¬¦å­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º "provider:model_name"
        
    Example:
        >>> # è·å–é»˜è®¤æ¨¡å‹å­—ç¬¦ä¸²
        >>> model_str = get_model_string()
        >>> print(model_str)  # "openai:gpt-4o"
        >>> 
        >>> # æŒ‡å®šæ¨¡å‹
        >>> model_str = get_model_string("gpt-4o-mini")
        >>> print(model_str)  # "openai:gpt-4o-mini"
        >>> 
        >>> # ä½¿ç”¨å…¶ä»–æä¾›å•†
        >>> model_str = get_model_string("claude-3-5-sonnet-20241022", "anthropic")
        >>> print(model_str)  # "anthropic:claude-3-5-sonnet-20241022"
    
    å‚è€ƒï¼š
        https://reference.langchain.com/python/langchain/models/
    """
    model_name = model_name or settings.openai_model
    model_string = f"{provider}:{model_name}"
    
    logger.debug(f"ğŸ”¤ ç”Ÿæˆæ¨¡å‹æ ‡è¯†ç¬¦: {model_string}")
    return model_string

