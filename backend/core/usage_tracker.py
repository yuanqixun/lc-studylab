"""
Token ä½¿ç”¨è¿½è¸ªå™¨
ç”¨äºŽè¿½è¸ª LLM çš„ token ä½¿ç”¨æƒ…å†µ,ä¸ºå‰ç«¯ Context ç»„ä»¶æä¾›æ•°æ®
"""

from typing import Dict, Optional, Any
from dataclasses import dataclass, field
from config import get_logger

logger = get_logger(__name__)


@dataclass
class TokenUsage:
    """Token ä½¿ç”¨ç»Ÿè®¡"""
    input_tokens: int = 0
    output_tokens: int = 0
    reasoning_tokens: int = 0
    cached_input_tokens: int = 0
    
    def to_dict(self) -> Dict[str, int]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "inputTokens": self.input_tokens,
            "outputTokens": self.output_tokens,
            "reasoningTokens": self.reasoning_tokens,
            "cachedInputTokens": self.cached_input_tokens,
        }


# å¸¸è§æ¨¡åž‹çš„ token é™åˆ¶
MODEL_LIMITS = {
    "gpt-4o": 128000,
    "gpt-4o-mini": 128000,
    "gpt-4-turbo": 128000,
    "gpt-4": 8192,
    "gpt-3.5-turbo": 16385,
    "claude-opus-4-20250514": 200000,
    "claude-sonnet-4-20250514": 200000,
    "claude-3-5-sonnet-20241022": 200000,
    "gemini-2.0-flash-exp": 1000000,
    "gemini-pro": 32768,
}


class UsageTracker:
    """
    è¿½è¸ª Agent æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ token ä½¿ç”¨æƒ…å†µ
    
    ç”¨æ³•:
        tracker = UsageTracker(model_id="gpt-4o")
        
        # åœ¨æµå¼è¾“å‡ºè¿‡ç¨‹ä¸­æ›´æ–°
        tracker.add_input_tokens(100)
        tracker.add_output_tokens(50)
        
        # èŽ·å–ä½¿ç”¨æƒ…å†µ
        usage_info = tracker.get_usage_info()
    """
    
    def __init__(self, model_id: str = "gpt-4o"):
        """
        åˆå§‹åŒ–è¿½è¸ªå™¨
        
        Args:
            model_id: æ¨¡åž‹æ ‡è¯†ç¬¦
        """
        self.model_id = model_id
        self.usage = TokenUsage()
        logger.debug(f"ðŸ“Š åˆå§‹åŒ– UsageTracker: model_id={model_id}")
    
    def add_input_tokens(self, count: int):
        """æ·»åŠ è¾“å…¥ token æ•°é‡"""
        self.usage.input_tokens += count
    
    def add_output_tokens(self, count: int):
        """æ·»åŠ è¾“å‡º token æ•°é‡"""
        self.usage.output_tokens += count
    
    def add_reasoning_tokens(self, count: int):
        """æ·»åŠ æŽ¨ç† token æ•°é‡ (ä»…éƒ¨åˆ†æ¨¡åž‹æ”¯æŒ)"""
        self.usage.reasoning_tokens += count
    
    def add_cached_tokens(self, count: int):
        """æ·»åŠ ç¼“å­˜å‘½ä¸­çš„ token æ•°é‡"""
        self.usage.cached_input_tokens += count
    
    def update_from_metadata(self, metadata: Dict[str, Any]):
        """
        ä»Ž LangChain çš„å…ƒæ•°æ®ä¸­æ›´æ–° token ä½¿ç”¨æƒ…å†µ
        
        Args:
            metadata: LangChain æ¶ˆæ¯çš„å…ƒæ•°æ®
        """
        if not metadata:
            return
        
        # LangChain çš„ token ä½¿ç”¨ä¿¡æ¯é€šå¸¸åœ¨ usage_metadata å­—æ®µ
        usage_meta = metadata.get("usage_metadata", {})
        
        if "input_tokens" in usage_meta:
            self.add_input_tokens(usage_meta["input_tokens"])
        
        if "output_tokens" in usage_meta:
            self.add_output_tokens(usage_meta["output_tokens"])
        
        if "reasoning_tokens" in usage_meta:
            self.add_reasoning_tokens(usage_meta["reasoning_tokens"])
        
        if "cached_tokens" in usage_meta:
            self.add_cached_tokens(usage_meta["cached_tokens"])
    
    def get_total_tokens(self) -> int:
        """èŽ·å–æ€» token æ•°"""
        return (
            self.usage.input_tokens 
            + self.usage.output_tokens 
            + self.usage.reasoning_tokens
        )
    
    def get_max_tokens(self) -> int:
        """èŽ·å–æ¨¡åž‹çš„æœ€å¤§ token é™åˆ¶"""
        return MODEL_LIMITS.get(self.model_id, 128000)
    
    def get_usage_percentage(self) -> float:
        """èŽ·å–ä½¿ç”¨ç™¾åˆ†æ¯”"""
        max_tokens = self.get_max_tokens()
        if max_tokens == 0:
            return 0.0
        return self.get_total_tokens() / max_tokens
    
    def get_usage_info(self) -> Dict[str, Any]:
        """
        èŽ·å–å®Œæ•´çš„ä½¿ç”¨æƒ…å†µä¿¡æ¯
        
        Returns:
            åŒ…å«æ‰€æœ‰ä½¿ç”¨ä¿¡æ¯çš„å­—å…¸,æ ¼å¼ç¬¦åˆå‰ç«¯ Context ç»„ä»¶è¦æ±‚
        """
        total_tokens = self.get_total_tokens()
        max_tokens = self.get_max_tokens()
        
        return {
            "usedTokens": total_tokens,
            "maxTokens": max_tokens,
            "usage": self.usage.to_dict(),
            "modelId": self.model_id,
            "percentage": self.get_usage_percentage(),
        }
    
    def log_summary(self):
        """æ‰“å°ä½¿ç”¨æƒ…å†µæ‘˜è¦"""
        info = self.get_usage_info()
        logger.info(
            f"ðŸ“Š Token ä½¿ç”¨ç»Ÿè®¡: "
            f"{info['usedTokens']}/{info['maxTokens']} "
            f"({info['percentage']:.1%}) - "
            f"è¾“å…¥:{self.usage.input_tokens}, "
            f"è¾“å‡º:{self.usage.output_tokens}"
        )


def create_usage_tracker(model_id: Optional[str] = None) -> UsageTracker:
    """
    åˆ›å»º UsageTracker çš„å·¥åŽ‚å‡½æ•°
    
    Args:
        model_id: æ¨¡åž‹æ ‡è¯†ç¬¦,å¦‚æžœä¸º None åˆ™ä½¿ç”¨é»˜è®¤å€¼
        
    Returns:
        UsageTracker å®žä¾‹
    """
    from config import settings
    
    if model_id is None:
        model_id = settings.openai_model
    
    return UsageTracker(model_id=model_id)

