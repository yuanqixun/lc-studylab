"""
DeepAgent æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“

è¿™æ˜¯ Stage 4 çš„æ ¸å¿ƒæ¨¡å—ï¼Œå®ç°åŸºäº LangChain 1.0.3 çš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è‡ªåŠ¨è§„åˆ’ç ”ç©¶ä»»åŠ¡
2. åè°ƒå¤šä¸ªå­æ™ºèƒ½ä½“
3. ç®¡ç†ç ”ç©¶æµç¨‹
4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

æŠ€æœ¯æ¶æ„ï¼š
- åŸºäº LangGraph æ„å»ºå·¥ä½œæµ
- ä½¿ç”¨ StateGraph ç®¡ç†çŠ¶æ€
- é›†æˆ SubAgents å¤„ç†å­ä»»åŠ¡
- ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ä¸­é—´ç»“æœ

å‚è€ƒï¼š
- https://docs.langchain.com/oss/python/deepagents/quickstart
- https://docs.langchain.com/oss/python/langgraph/quickstart
"""

from typing import Optional, List, Dict, Any, Sequence, TypedDict, Annotated
import json
from datetime import datetime

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import BaseTool
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite import SqliteSaver

from config import settings, get_logger
from core.models import get_chat_model
from core.tools.filesystem import get_filesystem, ResearchFileSystem
from core.prompts import WRITER_GUIDELINES
from core.guardrails.output_validators import OutputValidator
from deep_research.subagents import (
    create_web_researcher,
    create_doc_analyst,
    create_report_writer,
)

logger = get_logger(__name__)


# ==================== State å®šä¹‰ ====================

class ResearchState(TypedDict):
    """
    ç ”ç©¶çŠ¶æ€å®šä¹‰
    
    åŒ…å«ç ”ç©¶è¿‡ç¨‹ä¸­çš„æ‰€æœ‰çŠ¶æ€ä¿¡æ¯ã€‚
    
    Attributes:
        messages: æ¶ˆæ¯å†å²
        query: ç ”ç©¶é—®é¢˜
        thread_id: ç ”ç©¶ä»»åŠ¡ ID
        plan: ç ”ç©¶è®¡åˆ’
        web_research_done: ç½‘ç»œç ”ç©¶æ˜¯å¦å®Œæˆ
        doc_analysis_done: æ–‡æ¡£åˆ†ææ˜¯å¦å®Œæˆ
        report_done: æŠ¥å‘Šæ˜¯å¦å®Œæˆ
        current_step: å½“å‰æ­¥éª¤
        error: é”™è¯¯ä¿¡æ¯
        final_report: æœ€ç»ˆæŠ¥å‘Š
    """
    messages: Annotated[List[BaseMessage], add_messages]
    query: str
    thread_id: str
    plan: Optional[Dict[str, Any]]
    web_research_done: bool
    doc_analysis_done: bool
    report_done: bool
    current_step: str
    error: Optional[str]
    final_report: Optional[str]


# ==================== DeepAgent ç±» ====================

class DeepResearchAgent:
    """
    æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“
    
    åè°ƒå¤šä¸ªå­æ™ºèƒ½ä½“å®Œæˆå¤æ‚çš„ç ”ç©¶ä»»åŠ¡ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
    1. Planner: ç”Ÿæˆç ”ç©¶è®¡åˆ’
    2. WebResearcher: æœç´¢ç½‘ç»œä¿¡æ¯
    3. DocAnalyst: åˆ†ææ–‡æ¡£ï¼ˆå¯é€‰ï¼‰
    4. ReportWriter: æ’°å†™æœ€ç»ˆæŠ¥å‘Š
    
    Attributes:
        thread_id: ç ”ç©¶ä»»åŠ¡ ID
        filesystem: æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
        web_researcher: ç½‘ç»œç ”ç©¶å‘˜
        doc_analyst: æ–‡æ¡£åˆ†æå¸ˆï¼ˆå¯é€‰ï¼‰
        report_writer: æŠ¥å‘Šæ’°å†™è€…
        graph: LangGraph å·¥ä½œæµ
        
    Example:
        >>> agent = DeepResearchAgent(
        ...     thread_id="research_123",
        ...     enable_doc_analysis=True
        ... )
        >>> result = agent.research("åˆ†æ LangChain 1.0 çš„æ–°ç‰¹æ€§")
        >>> print(result["final_report"])
    """
    
    def __init__(
        self,
        thread_id: str,
        enable_web_search: bool = True,
        enable_doc_analysis: bool = False,
        retriever_tool: Optional[BaseTool] = None,
        checkpointer: Optional[Any] = None,
        **kwargs,
    ):
        """
        åˆå§‹åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“
        
        Args:
            thread_id: ç ”ç©¶ä»»åŠ¡çš„å”¯ä¸€æ ‡è¯†ç¬¦
            enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
            enable_doc_analysis: æ˜¯å¦å¯ç”¨æ–‡æ¡£åˆ†æ
            retriever_tool: RAG æ£€ç´¢å·¥å…·ï¼ˆå¦‚æœå¯ç”¨æ–‡æ¡£åˆ†æï¼‰
            checkpointer: çŠ¶æ€æ£€æŸ¥ç‚¹ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        """
        self.thread_id = thread_id
        self.enable_web_search = enable_web_search
        self.enable_doc_analysis = enable_doc_analysis
        
        logger.info(f"ğŸš€ åˆå§‹åŒ– DeepResearchAgent: {thread_id}")
        logger.info(f"   ç½‘ç»œæœç´¢: {enable_web_search}")
        logger.info(f"   æ–‡æ¡£åˆ†æ: {enable_doc_analysis}")
        
        # åˆå§‹åŒ–æ–‡ä»¶ç³»ç»Ÿ
        self.filesystem = get_filesystem(thread_id)
        
        # åˆ›å»ºå­æ™ºèƒ½ä½“
        self._init_subagents(retriever_tool)
        
        # åˆ›å»ºå·¥ä½œæµ
        self.graph = self._build_graph(checkpointer)
        
        logger.info("âœ… DeepResearchAgent åˆå§‹åŒ–å®Œæˆ")
    
    def _init_subagents(self, retriever_tool: Optional[BaseTool] = None) -> None:
        """
        åˆå§‹åŒ–å­æ™ºèƒ½ä½“
        
        Args:
            retriever_tool: RAG æ£€ç´¢å·¥å…·
        """
        logger.info("ğŸ¤– åˆå§‹åŒ–å­æ™ºèƒ½ä½“...")
        
        # WebResearcher
        if self.enable_web_search:
            self.web_researcher = create_web_researcher()
            logger.debug("   âœ“ WebResearcher")
        else:
            self.web_researcher = None
            logger.debug("   âœ— WebResearcher (ç¦ç”¨)")
        
        # DocAnalyst
        if self.enable_doc_analysis:
            if retriever_tool is None:
                logger.warning("âš ï¸ å¯ç”¨äº†æ–‡æ¡£åˆ†æä½†æœªæä¾› retriever_tool")
            self.doc_analyst = create_doc_analyst(retriever_tool=retriever_tool)
            logger.debug("   âœ“ DocAnalyst")
        else:
            self.doc_analyst = None
            logger.debug("   âœ— DocAnalyst (ç¦ç”¨)")
        
        # ReportWriterï¼ˆæ€»æ˜¯éœ€è¦ï¼‰
        self.report_writer = create_report_writer()
        logger.debug("   âœ“ ReportWriter")
    
    def _build_graph(self, checkpointer: Optional[Any] = None) -> Any:
        """
        æ„å»º LangGraph å·¥ä½œæµ
        
        Args:
            checkpointer: çŠ¶æ€æ£€æŸ¥ç‚¹
            
        Returns:
            ç¼–è¯‘åçš„ StateGraph
        """
        logger.info("ğŸ”¨ æ„å»ºç ”ç©¶å·¥ä½œæµ...")
        
        # åˆ›å»º StateGraph
        workflow = StateGraph(ResearchState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("planner", self._planner_node)
        
        if self.enable_web_search:
            workflow.add_node("web_research", self._web_research_node)
        
        if self.enable_doc_analysis:
            workflow.add_node("doc_analysis", self._doc_analysis_node)
        
        workflow.add_node("report_writing", self._report_writing_node)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("planner")
        
        # æ·»åŠ è¾¹
        if self.enable_web_search:
            workflow.add_edge("planner", "web_research")
            
            if self.enable_doc_analysis:
                workflow.add_edge("web_research", "doc_analysis")
                workflow.add_edge("doc_analysis", "report_writing")
            else:
                workflow.add_edge("web_research", "report_writing")
        else:
            if self.enable_doc_analysis:
                workflow.add_edge("planner", "doc_analysis")
                workflow.add_edge("doc_analysis", "report_writing")
            else:
                workflow.add_edge("planner", "report_writing")
        
        workflow.add_edge("report_writing", END)
        
        # ç¼–è¯‘
        if checkpointer is None:
            # ä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹
            from langgraph.checkpoint.memory import MemorySaver
            checkpointer = MemorySaver()
        
        graph = workflow.compile(checkpointer=checkpointer)
        
        logger.info("âœ… å·¥ä½œæµæ„å»ºå®Œæˆ")
        return graph
    
    # ==================== èŠ‚ç‚¹å‡½æ•° ====================
    
    def _planner_node(self, state: ResearchState) -> ResearchState:
        """
        è§„åˆ’èŠ‚ç‚¹ï¼šç”Ÿæˆç ”ç©¶è®¡åˆ’
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        logger.info("ğŸ“‹ æ‰§è¡Œè§„åˆ’èŠ‚ç‚¹...")
        
        query = state["query"]
        thread_id = state["thread_id"]
        
        # ç”Ÿæˆç ”ç©¶è®¡åˆ’
        plan_prompt = f"""è¯·ä¸ºä»¥ä¸‹ç ”ç©¶é—®é¢˜åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’ï¼š

ç ”ç©¶é—®é¢˜ï¼š{query}

å¯ç”¨èµ„æºï¼š
- ç½‘ç»œæœç´¢ï¼š{"æ˜¯" if self.enable_web_search else "å¦"}
- æ–‡æ¡£åˆ†æï¼š{"æ˜¯" if self.enable_doc_analysis else "å¦"}

è¯·è¾“å‡º JSON æ ¼å¼çš„ç ”ç©¶è®¡åˆ’ï¼š
{{
    "research_goal": "ç ”ç©¶ç›®æ ‡",
    "key_questions": ["é—®é¢˜1", "é—®é¢˜2", ...],
    "search_keywords": ["å…³é”®è¯1", "å…³é”®è¯2", ...],
    "expected_outcomes": ["é¢„æœŸæˆæœ1", "é¢„æœŸæˆæœ2", ...]
}}
"""
        
        # ä½¿ç”¨ LLM ç”Ÿæˆè®¡åˆ’
        try:
            model = get_chat_model()
            response = model.invoke([HumanMessage(content=plan_prompt)])
            
            # è§£æ JSON
            plan_text = response.content
            
            # å°è¯•æå– JSON
            import re
            json_match = re.search(r'\{.*\}', plan_text, re.DOTALL)
            if json_match:
                plan = json.loads(json_match.group())
            else:
                # å¦‚æœæ²¡æœ‰ JSONï¼Œåˆ›å»ºé»˜è®¤è®¡åˆ’
                plan = {
                    "research_goal": query,
                    "key_questions": [query],
                    "search_keywords": query.split(),
                    "expected_outcomes": ["å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Š"]
                }
            
            # ä¿å­˜è®¡åˆ’
            plan_content = f"""# ç ”ç©¶è®¡åˆ’

## ç ”ç©¶ç›®æ ‡
{plan.get('research_goal', query)}

## å…³é”®é—®é¢˜
{chr(10).join([f"- {q}" for q in plan.get('key_questions', [query])])}

## æœç´¢å…³é”®è¯
{', '.join(plan.get('search_keywords', []))}

## é¢„æœŸæˆæœ
{chr(10).join([f"- {o}" for o in plan.get('expected_outcomes', [])])}

---
ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            
            self.filesystem.write_file(
                "research_plan.md",
                plan_content,
                subdirectory="plans"
            )
            
            logger.info("âœ… ç ”ç©¶è®¡åˆ’å·²ç”Ÿæˆ")
            
            # æ›´æ–°çŠ¶æ€
            state["plan"] = plan
            state["current_step"] = "planner"
            state["messages"].append(AIMessage(content=f"ç ”ç©¶è®¡åˆ’å·²ç”Ÿæˆï¼š{plan.get('research_goal')}"))
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆè®¡åˆ’å¤±è´¥: {e}")
            state["error"] = str(e)
            state["plan"] = {"research_goal": query}
        
        return state
    
    def _web_research_node(self, state: ResearchState) -> ResearchState:
        """
        ç½‘ç»œç ”ç©¶èŠ‚ç‚¹ï¼šæœç´¢å’Œæ•´ç†ç½‘ç»œä¿¡æ¯
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        logger.info("ğŸ” æ‰§è¡Œç½‘ç»œç ”ç©¶èŠ‚ç‚¹...")
        
        query = state["query"]
        thread_id = state["thread_id"]
        plan = state.get("plan", {})
        
        # æ„å»ºç ”ç©¶æŒ‡ä»¤
        research_instruction = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ·±å…¥çš„ç½‘ç»œç ”ç©¶ï¼š

ç ”ç©¶é—®é¢˜ï¼š{query}

ç ”ç©¶è®¡åˆ’ï¼š
{json.dumps(plan, ensure_ascii=False, indent=2)}

ä»»åŠ¡è¦æ±‚ï¼š
1. ä½¿ç”¨æœç´¢å·¥å…·æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯
2. è¯„ä¼°ä¿¡æ¯çš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§
3. æå–å…³é”®ä¿¡æ¯å’Œæ•°æ®
4. æ•´ç†ä¸ºè¦ç‚¹ä¸æ®µè½æ··åˆçš„ç ”ç©¶ç¬”è®°ï¼ŒæŒ‰æ¥æºç±»å‹è‡ªé€‚é…å‘ˆç°ï¼ˆå®˜æ–¹æ–‡æ¡£ã€è®ºæ–‡ã€æ ‡å‡†ã€æ–°é—»ã€åšå®¢ï¼‰
5. ä½¿ç”¨å†…è”å¼•ç”¨å¹¶åœ¨ç»“å°¾åˆ—å‡ºå‚è€ƒæ¥æº
6. ä½¿ç”¨ write_research_file ä¿å­˜åˆ° notes/web_research.md

å†™ä½œå‡†åˆ™ï¼š
{WRITER_GUIDELINES}

thread_id: {thread_id}
"""
        
        try:
            # è°ƒç”¨ WebResearcher
            result = self.web_researcher.invoke({
                "messages": [HumanMessage(content=research_instruction)]
            })
            
            # éªŒè¯ç¬”è®°æ˜¯å¦å·²ä¿å­˜ï¼ˆæ–‡ä»¶ç³»ç»Ÿå·²ä¿è¯åŒæ­¥å†™å…¥ï¼‰
            notes_saved = False
            try:
                notes = self.filesystem.read_file("web_research.md", subdirectory="notes")
                notes_saved = True
                logger.info("âœ… ç½‘ç»œç ”ç©¶ç¬”è®°å·²ä¿å­˜")
            except Exception:
                logger.debug("   æœªåœ¨æ–‡ä»¶ç³»ç»Ÿä¸­æ‰¾åˆ°ç¬”è®°ï¼Œå°è¯•ä» Agent è¾“å‡ºæå–...")
            
            # å¦‚æœç¬”è®°æ²¡æœ‰ä¿å­˜ï¼Œå°è¯•ä» Agent è¾“å‡ºä¸­æå–
            if not notes_saved:
                if isinstance(result, dict) and "messages" in result:
                    messages = result["messages"]
                    
                    # æ”¶é›†æ‰€æœ‰ AI æ¶ˆæ¯å†…å®¹
                    research_content = []
                    for msg in messages:
                        if isinstance(msg, AIMessage) and msg.content:
                            # è·³è¿‡å·¥å…·è°ƒç”¨çš„æ¶ˆæ¯
                            if not msg.content.startswith("æ‰¾åˆ°") and not msg.content.startswith("æœç´¢"):
                                research_content.append(msg.content)
                    
                    if research_content:
                        # åˆå¹¶å†…å®¹å¹¶ä¿å­˜
                        combined_content = "\n\n".join(research_content)
                        
                        # å¦‚æœå†…å®¹ä¸æ˜¯ Markdown æ ¼å¼ï¼Œæ·»åŠ æ ‡é¢˜
                        if not combined_content.strip().startswith("#"):
                            combined_content = f"""# ç ”ç©¶ç¬”è®°ï¼š{query}

## ç ”ç©¶å†…å®¹

{combined_content}

## è¯´æ˜
æœ¬ç¬”è®°ç”±ç³»ç»Ÿè‡ªåŠ¨ä»ç ”ç©¶è¿‡ç¨‹ä¸­æå–ã€‚

---
*ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
                        
                        try:
                            self.filesystem.write_file(
                                "web_research.md",
                                combined_content,
                                subdirectory="notes",
                                metadata={"source": "agent_output_extraction"}
                            )
                            logger.info("âœ… å·²ä» Agent è¾“å‡ºæå–å¹¶ä¿å­˜ç ”ç©¶ç¬”è®°")
                        except Exception as save_error:
                            logger.error(f"âŒ ä¿å­˜æå–çš„ç¬”è®°å¤±è´¥: {save_error}")
            
            logger.info("âœ… ç½‘ç»œç ”ç©¶å®Œæˆ")
            
            # æ›´æ–°çŠ¶æ€
            state["web_research_done"] = True
            state["current_step"] = "web_research"
            state["messages"].append(AIMessage(content="ç½‘ç»œç ”ç©¶å·²å®Œæˆ"))
            
        except Exception as e:
            logger.error(f"âŒ ç½‘ç»œç ”ç©¶å¤±è´¥: {e}")
            state["error"] = str(e)
        
        return state
    
    def _doc_analysis_node(self, state: ResearchState) -> ResearchState:
        """
        æ–‡æ¡£åˆ†æèŠ‚ç‚¹ï¼šåˆ†ææœ¬åœ°æ–‡æ¡£
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        logger.info("ğŸ“š æ‰§è¡Œæ–‡æ¡£åˆ†æèŠ‚ç‚¹...")
        
        query = state["query"]
        thread_id = state["thread_id"]
        plan = state.get("plan", {})
        
        # æ„å»ºåˆ†ææŒ‡ä»¤
        analysis_instruction = f"""è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ·±å…¥çš„æ–‡æ¡£åˆ†æï¼š

ç ”ç©¶é—®é¢˜ï¼š{query}

ç ”ç©¶è®¡åˆ’ï¼š
{json.dumps(plan, ensure_ascii=False, indent=2)}

ä»»åŠ¡è¦æ±‚ï¼š
1. ä½¿ç”¨ knowledge_base å·¥å…·æ£€ç´¢ç›¸å…³æ–‡æ¡£
2. åˆ†ææ–‡æ¡£å†…å®¹çš„ç›¸å…³æ€§
3. æç‚¼å…³é”®æ®µè½å’Œæ•°æ®
4. æ•´ç†æˆç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Š
5. ä½¿ç”¨ write_research_file ä¿å­˜æŠ¥å‘Šåˆ° notes ç›®å½•ï¼Œæ–‡ä»¶åï¼šdoc_analysis.md

è¯·ç¡®ä¿ï¼š
- æ£€ç´¢å¤šä¸ªç›¸å…³æ–‡æ¡£
- ç›´æ¥å¼•ç”¨åŸæ–‡
- è®°å½•æ–‡æ¡£æ¥æº
- æç‚¼æ ¸å¿ƒè§‚ç‚¹

thread_id: {thread_id}
"""
        
        try:
            # è°ƒç”¨ DocAnalyst
            result = self.doc_analyst.invoke({
                "messages": [HumanMessage(content=analysis_instruction)]
            })
            
            logger.info("âœ… æ–‡æ¡£åˆ†æå®Œæˆ")
            
            # æ›´æ–°çŠ¶æ€
            state["doc_analysis_done"] = True
            state["current_step"] = "doc_analysis"
            state["messages"].append(AIMessage(content="æ–‡æ¡£åˆ†æå·²å®Œæˆ"))
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£åˆ†æå¤±è´¥: {e}")
            state["error"] = str(e)
        
        return state
    
    def _report_writing_node(self, state: ResearchState) -> ResearchState:
        """
        æŠ¥å‘Šæ’°å†™èŠ‚ç‚¹ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        
        Args:
            state: å½“å‰çŠ¶æ€
            
        Returns:
            æ›´æ–°åçš„çŠ¶æ€
        """
        logger.info("âœï¸ æ‰§è¡ŒæŠ¥å‘Šæ’°å†™èŠ‚ç‚¹...")
        
        query = state["query"]
        thread_id = state["thread_id"]
        
        # æ„å»ºæ’°å†™æŒ‡ä»¤
        writing_instruction = f"""è¯·æ ¹æ®æ‰€æœ‰ç ”ç©¶ææ–™æ’°å†™æœ€ç»ˆç ”ç©¶æŠ¥å‘Šï¼š

ç ”ç©¶é—®é¢˜ï¼š{query}

ä»»åŠ¡è¦æ±‚ï¼š
1. ä½¿ç”¨ list_research_files åˆ—å‡ºç ”ç©¶ç¬”è®°ï¼ˆthread_id: {thread_id}ï¼‰å¹¶é€ä¸€è¯»å–
2. æ•´åˆç ”ç©¶å‘ç°ä¸è¯æ®ï¼Œé¿å…æ¨¡æ¿åŒ–ç»“æ„ï¼ŒæŒ‰ä¸»é¢˜ä¸ä¿¡æ¯å¯†åº¦é€‰æ‹©åˆé€‚ç« èŠ‚
3. æä¾›çœŸå®ç¤ºä¾‹æˆ–ä»£ç ç‰‡æ®µï¼ˆæŠ€æœ¯ä¸»é¢˜ï¼‰ä¸å®è·µå»ºè®®
4. ä½¿ç”¨å†…è”å¼•ç”¨å¹¶åœ¨ç»“å°¾åˆ—å‡ºå‚è€ƒæ¥æº
5. ä½¿ç”¨ write_research_file ä¿å­˜åˆ° reports/final_report.md

å†™ä½œå‡†åˆ™ï¼š
{WRITER_GUIDELINES}

thread_id: {thread_id}
"""
        
        try:
            # è°ƒç”¨ ReportWriter
            result = self.report_writer.invoke({
                "messages": [HumanMessage(content=writing_instruction)]
            })
            
            # å°è¯•ä»å¤šä¸ªæ¥æºè·å–æŠ¥å‘Šï¼ˆæ–‡ä»¶ç³»ç»Ÿå·²ä¿è¯åŒæ­¥å†™å…¥ï¼‰
            final_report = None
            
            # 1. å…ˆå°è¯•ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–
            try:
                final_report = self.filesystem.read_file(
                    "final_report.md",
                    subdirectory="reports"
                )
                logger.info("âœ… ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æœ€ç»ˆæŠ¥å‘Š")
            except Exception as e:
                logger.debug(f"   æ— æ³•ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–: {e}")
            
            # 2. å¦‚æœæ–‡ä»¶ç³»ç»Ÿæ²¡æœ‰ï¼Œå°è¯•ä» Agent çš„è¾“å‡ºä¸­æå–
            if not final_report:
                logger.info("ğŸ“ ä» Agent è¾“å‡ºä¸­æå–æŠ¥å‘Šå†…å®¹...")
                
                # ä» result ä¸­æå– AI æ¶ˆæ¯
                if isinstance(result, dict) and "messages" in result:
                    messages = result["messages"]
                    
                    # æ”¶é›†æ‰€æœ‰ AI æ¶ˆæ¯å†…å®¹ï¼ˆå¯èƒ½åŒ…å«æŠ¥å‘Šï¼‰
                    ai_contents = []
                    for msg in messages:
                        if isinstance(msg, AIMessage) and msg.content:
                            # è·³è¿‡å·¥å…·è°ƒç”¨ç»“æœæ¶ˆæ¯
                            content = msg.content.strip()
                            if content and not content.startswith("æ‰¾åˆ°") and not content.startswith("æ–‡ä»¶å·²ä¿å­˜"):
                                ai_contents.append(content)
                    
                    # å°è¯•æ‰¾åˆ°æœ€é•¿çš„ã€çœ‹èµ·æ¥åƒæŠ¥å‘Šçš„å†…å®¹
                    for content in sorted(ai_contents, key=len, reverse=True):
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ¥å‘Šç‰¹å¾
                        is_report = (
                            len(content) > 200 and  # è¶³å¤Ÿé•¿
                            (content.startswith("#") or  # Markdown æ ‡é¢˜
                             "##" in content or  # åŒ…å«äºŒçº§æ ‡é¢˜
                             "æ‰§è¡Œæ‘˜è¦" in content or  # åŒ…å«æŠ¥å‘Šå…³é”®è¯
                             "ç ”ç©¶èƒŒæ™¯" in content or
                             "ä¸»è¦å‘ç°" in content)
                        )
                        
                        if is_report:
                            final_report = content
                            logger.info(f"âœ… ä» Agent è¾“å‡ºä¸­æå–åˆ°æŠ¥å‘Šï¼ˆé•¿åº¦: {len(content)} å­—ç¬¦ï¼‰")
                            
                            # ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
                            try:
                                self.filesystem.write_file(
                                    "final_report.md",
                                    final_report,
                                    subdirectory="reports",
                                    metadata={"source": "agent_output"}
                                )
                                logger.info("âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ")
                            except Exception as save_error:
                                logger.warning(f"âš ï¸ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {save_error}")
                            
                            break
            
            # 3. å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œç”Ÿæˆç»¼åˆæŠ¥å‘Š
            if not final_report:
                logger.info("ğŸ“‹ Agent æœªç›´æ¥ç”ŸæˆæŠ¥å‘Šï¼Œä½¿ç”¨ç ”ç©¶ææ–™ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
                
                # è¯»å–æ‰€æœ‰å¯ç”¨çš„ç ”ç©¶ææ–™
                research_materials = []
                
                # å°è¯•è¯»å–ç ”ç©¶è®¡åˆ’
                try:
                    plan_content = self.filesystem.read_file(
                        "research_plan.md",
                        subdirectory="plans"
                    )
                    research_materials.append(("ç ”ç©¶è®¡åˆ’", plan_content))
                    logger.debug("   âœ“ è¯»å–ç ”ç©¶è®¡åˆ’")
                except Exception:
                    logger.debug("   âœ— æœªæ‰¾åˆ°ç ”ç©¶è®¡åˆ’")
                
                # å°è¯•è¯»å–ç½‘ç»œç ”ç©¶ç¬”è®°
                try:
                    web_notes = self.filesystem.read_file(
                        "web_research.md",
                        subdirectory="notes"
                    )
                    research_materials.append(("ç½‘ç»œç ”ç©¶ç¬”è®°", web_notes))
                    logger.debug("   âœ“ è¯»å–ç½‘ç»œç ”ç©¶ç¬”è®°")
                except Exception:
                    logger.debug("   âœ— æœªæ‰¾åˆ°ç½‘ç»œç ”ç©¶ç¬”è®°")
                
                # å°è¯•è¯»å–æ–‡æ¡£åˆ†ææŠ¥å‘Š
                try:
                    doc_notes = self.filesystem.read_file(
                        "doc_analysis.md",
                        subdirectory="notes"
                    )
                    research_materials.append(("æ–‡æ¡£åˆ†ææŠ¥å‘Š", doc_notes))
                    logger.debug("   âœ“ è¯»å–æ–‡æ¡£åˆ†ææŠ¥å‘Š")
                except Exception:
                    logger.debug("   âœ— æœªæ‰¾åˆ°æ–‡æ¡£åˆ†ææŠ¥å‘Š")
                
                # ç”ŸæˆåŸºç¡€æŠ¥å‘Š
                if research_materials:
                    logger.info(f"   æ‰¾åˆ° {len(research_materials)} ä¸ªç ”ç©¶ææ–™ï¼Œç”Ÿæˆç»¼åˆæŠ¥å‘Š")
                    
                    # æ„å»ºæŠ¥å‘Šå†…å®¹
                    materials_section = ""
                    for title, content in research_materials:
                        materials_section += f"\n### {title}\n\n{content}\n\n"
                    
                    final_report = f"""# {query}

{materials_section}

ç»“è®ºä¸å»ºè®®ï¼šåŸºäºä¸Šè¿°ææ–™ç»™å‡ºæ¸…æ™°ç»“è®ºä¸å¯æ‰§è¡Œå»ºè®®ï¼Œå¹¶åœ¨æ–‡ä¸­ä¿ç•™å…³é”®è¯æ®çš„å†…è”å¼•ç”¨ã€‚

å‚è€ƒæ¥æºï¼šè¯·åœ¨æ–‡æœ«åˆ—å‡ºæ¥æºã€‚

ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ç ”ç©¶ä»»åŠ¡IDï¼š{thread_id}
"""
                else:
                    logger.warning("   æœªæ‰¾åˆ°ä»»ä½•ç ”ç©¶ææ–™")
                    final_report = f"""# {query}

## æ‰§è¡Œæ‘˜è¦

æœ¬ç ”ç©¶é’ˆå¯¹"{query}"è¿›è¡Œäº†è°ƒç ”ã€‚

## è¯´æ˜

ç ”ç©¶è¿‡ç¨‹å·²å®Œæˆï¼Œä½†æœªèƒ½æ‰¾åˆ°ä¿å­˜çš„ç ”ç©¶ææ–™ã€‚è¿™å¯èƒ½æ˜¯ç”±äºï¼š
1. ç ”ç©¶ä»»åŠ¡åˆšåˆšå¯åŠ¨ï¼Œææ–™å°šæœªç”Ÿæˆ
2. æ–‡ä»¶ä¿å­˜è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜
3. ç ”ç©¶å·¥å…·æœªèƒ½æ­£ç¡®è°ƒç”¨

å»ºè®®ï¼š
- æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦ç»†æƒ…å†µ
- é‡æ–°è¿è¡Œç ”ç©¶ä»»åŠ¡
- æ£€æŸ¥ API é…ç½®å’Œç½‘ç»œè¿æ¥

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*ç ”ç©¶ä»»åŠ¡ IDï¼š{thread_id}*
"""
                
                # ä¿å­˜åŸºç¡€æŠ¥å‘Š
                try:
                    self.filesystem.write_file(
                        "final_report.md",
                        final_report,
                        subdirectory="reports",
                        metadata={
                            "source": "fallback",
                            "materials_count": len(research_materials)
                        }
                    )
                    logger.info("âœ… åŸºç¡€æŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜")
                except Exception as save_error:
                    logger.error(f"âŒ ä¿å­˜åŸºç¡€æŠ¥å‘Šå¤±è´¥: {save_error}")
            
            is_technical = any(w in query.lower() for w in ["react", "hook", "api", "ç¼–ç¨‹", "ä»£ç ", "javascript", "python"])            
            validator = OutputValidator(require_examples=is_technical)
            result = validator.validate(final_report)
            if not result.is_valid:
                try:
                    revision_prompt = f"è¯·åœ¨ä¿æŒç°æœ‰ç»“æ„ä¸å¼•ç”¨çš„å‰æä¸‹ï¼Œè¡¥å……ç¤ºä¾‹æˆ–ä»£ç ç‰‡æ®µï¼Œå¹¶æå‡ä¿¡æ¯å¯†åº¦ä¸å¯æ“ä½œæ€§ã€‚\n\nå†™ä½œå‡†åˆ™ï¼š\n{WRITER_GUIDELINES}\n\nåŸæ–‡ï¼š\n{final_report}"
                    model = get_chat_model()
                    revised = model.invoke([HumanMessage(content=revision_prompt)])
                    revised_text = revised.content or final_report
                    final_report = revised_text
                    self.filesystem.write_file(
                        "final_report.md",
                        final_report,
                        subdirectory="reports",
                        metadata={"source": "revision"}
                    )
                except Exception:
                    pass

            state["final_report"] = final_report
            state["report_done"] = True
            state["current_step"] = "report_writing"
            state["messages"].append(AIMessage(content="æœ€ç»ˆæŠ¥å‘Šå·²å®Œæˆ"))
            
            logger.info("âœ… æŠ¥å‘Šæ’°å†™èŠ‚ç‚¹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æŠ¥å‘Šæ’°å†™å¤±è´¥: {e}")
            state["error"] = str(e)
        
        return state
    
    # ==================== å…¬å…±æ–¹æ³• ====================
    
    def research(
        self,
        query: str,
        config: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œç ”ç©¶ä»»åŠ¡
        
        Args:
            query: ç ”ç©¶é—®é¢˜
            config: é…ç½®å‚æ•°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç ”ç©¶ç»“æœå­—å…¸
            
        Example:
            >>> agent = DeepResearchAgent(thread_id="research_123")
            >>> result = agent.research("åˆ†æ LangChain 1.0 çš„æ–°ç‰¹æ€§")
            >>> print(result["final_report"])
        """
        logger.info(f"ğŸš€ å¼€å§‹ç ”ç©¶ä»»åŠ¡: {query}")
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state: ResearchState = {
            "messages": [HumanMessage(content=query)],
            "query": query,
            "thread_id": self.thread_id,
            "plan": None,
            "web_research_done": False,
            "doc_analysis_done": False,
            "report_done": False,
            "current_step": "init",
            "error": None,
            "final_report": None,
        }
        
        # æ‰§è¡Œå·¥ä½œæµ
        try:
            if config is None:
                config = {"configurable": {"thread_id": self.thread_id}}
            
            final_state = self.graph.invoke(initial_state, config)
            
            logger.info("âœ… ç ”ç©¶ä»»åŠ¡å®Œæˆ")
            
            # è¿”å›ç»“æœ
            return {
                "status": "completed",
                "query": query,
                "thread_id": self.thread_id,
                "final_report": final_state.get("final_report"),
                "plan": final_state.get("plan"),
                "error": final_state.get("error"),
                "steps_completed": {
                    "web_research": final_state.get("web_research_done", False),
                    "doc_analysis": final_state.get("doc_analysis_done", False),
                    "report": final_state.get("report_done", False),
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ç ”ç©¶ä»»åŠ¡å¤±è´¥: {e}")
            return {
                "status": "failed",
                "query": query,
                "thread_id": self.thread_id,
                "error": str(e),
            }
    
    def get_status(self) -> Dict[str, Any]:
        """
        è·å–ç ”ç©¶çŠ¶æ€
        
        Returns:
            çŠ¶æ€ä¿¡æ¯å­—å…¸
        """
        # åˆ—å‡ºæ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶
        files = self.filesystem.list_files()
        
        return {
            "thread_id": self.thread_id,
            "filesystem_files": files,
            "web_search_enabled": self.enable_web_search,
            "doc_analysis_enabled": self.enable_doc_analysis,
        }


# ==================== å·¥å‚å‡½æ•° ====================

def create_deep_research_agent(
    thread_id: str,
    enable_web_search: bool = True,
    enable_doc_analysis: bool = False,
    retriever_tool: Optional[BaseTool] = None,
    **kwargs,
) -> DeepResearchAgent:
    """
    åˆ›å»ºæ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“çš„ä¾¿æ·å·¥å‚å‡½æ•°
    
    Args:
        thread_id: ç ”ç©¶ä»»åŠ¡ ID
        enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
        enable_doc_analysis: æ˜¯å¦å¯ç”¨æ–‡æ¡£åˆ†æ
        retriever_tool: RAG æ£€ç´¢å·¥å…·
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        DeepResearchAgent å®ä¾‹
        
    Example:
        >>> agent = create_deep_research_agent(
        ...     thread_id="research_123",
        ...     enable_web_search=True,
        ...     enable_doc_analysis=True,
        ...     retriever_tool=my_retriever_tool
        ... )
        >>> result = agent.research("åˆ†æ AI é¢†åŸŸçš„æœ€æ–°è¶‹åŠ¿")
    """
    return DeepResearchAgent(
        thread_id=thread_id,
        enable_web_search=enable_web_search,
        enable_doc_analysis=enable_doc_analysis,
        retriever_tool=retriever_tool,
        **kwargs,
    )


logger.info("âœ… DeepAgent æ¨¡å—å·²åŠ è½½")
