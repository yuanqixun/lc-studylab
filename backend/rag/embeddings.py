"""
Embeddings æ¨¡å—

æä¾›ç»Ÿä¸€çš„ Embedding æ¨¡å‹æ¥å£ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ã€‚

æ”¯æŒçš„ Embedding æ¨¡å‹ï¼š
- OpenAI Embeddings (text-embedding-3-small, text-embedding-3-large)
- å¯æ‰©å±•æ”¯æŒå…¶ä»– embedding æ¨¡å‹

å‚è€ƒï¼š
- https://reference.langchain.com/python/langchain_core/embeddings/
- https://reference.langchain.com/python/langchain_openai/embeddings/
"""

from typing import Optional, List
from langchain_openai import OpenAIEmbeddings
from langchain_core.embeddings import Embeddings
import tiktoken

from config import settings, get_logger

logger = get_logger(__name__)


class SafeOpenAIEmbeddings(Embeddings):
    """
    å®‰å…¨çš„ OpenAI Embeddings åŒ…è£…å™¨
    
    è‡ªåŠ¨å¤„ç† token é™åˆ¶é—®é¢˜ï¼Œå¯¹è¶…é•¿æ–‡æœ¬è¿›è¡Œæˆªæ–­
    """
    
    def __init__(
        self,
        embeddings: OpenAIEmbeddings,
        max_tokens: int = 512,
        encoding_name: str = "cl100k_base",
    ):
        """
        åˆå§‹åŒ–å®‰å…¨çš„ Embeddings åŒ…è£…å™¨
        
        Args:
            embeddings: åŸå§‹çš„ OpenAIEmbeddings å®ä¾‹
            max_tokens: æœ€å¤§ token æ•°é™åˆ¶
            encoding_name: tokenizer ç¼–ç åç§°
        """
        self.embeddings = embeddings
        self.max_tokens = max_tokens
        try:
            self.encoding = tiktoken.get_encoding(encoding_name)
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½ tiktoken ç¼–ç å™¨: {e}ï¼Œä½¿ç”¨ç®€å•å­—ç¬¦æˆªæ–­")
            self.encoding = None
    
    def _truncate_text(self, text: str) -> str:
        """
        æˆªæ–­æ–‡æœ¬åˆ°æœ€å¤§ token é™åˆ¶
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æˆªæ–­åçš„æ–‡æœ¬
        """
        if not text:
            return text
            
        if self.encoding is None:
            # å¦‚æœæ²¡æœ‰ tokenizerï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦æˆªæ–­
            # å‡è®¾å¹³å‡æ¯ä¸ª token çº¦ 4 ä¸ªå­—ç¬¦ï¼ˆä¸­æ–‡çº¦ 1.5-2 ä¸ªå­—ç¬¦ï¼‰
            max_chars = self.max_tokens * 2  # ä¿å®ˆä¼°è®¡
            if len(text) > max_chars:
                logger.warning(f"æ–‡æœ¬è¿‡é•¿ ({len(text)} å­—ç¬¦)ï¼Œæˆªæ–­åˆ° {max_chars} å­—ç¬¦")
                return text[:max_chars]
            return text
        
        # ä½¿ç”¨ tiktoken è¿›è¡Œç²¾ç¡®çš„ token è®¡æ•°å’Œæˆªæ–­
        tokens = self.encoding.encode(text)
        if len(tokens) > self.max_tokens:
            logger.warning(
                f"æ–‡æœ¬è¿‡é•¿ ({len(tokens)} tokens)ï¼Œæˆªæ–­åˆ° {self.max_tokens} tokens"
            )
            truncated_tokens = tokens[:self.max_tokens]
            return self.encoding.decode(truncated_tokens)
        
        return text
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡åµŒå…¥æ–‡æ¡£ï¼Œè‡ªåŠ¨æˆªæ–­è¿‡é•¿æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        # æˆªæ–­æ‰€æœ‰æ–‡æœ¬
        truncated_texts = [self._truncate_text(text) for text in texts]
        
        # è°ƒç”¨åŸå§‹çš„ embeddings
        return self.embeddings.embed_documents(truncated_texts)
    
    def embed_query(self, text: str) -> List[float]:
        """
        åµŒå…¥æŸ¥è¯¢æ–‡æœ¬ï¼Œè‡ªåŠ¨æˆªæ–­è¿‡é•¿æ–‡æœ¬
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            å‘é‡
        """
        # æˆªæ–­æ–‡æœ¬
        truncated_text = self._truncate_text(text)
        
        # è°ƒç”¨åŸå§‹çš„ embeddings
        return self.embeddings.embed_query(truncated_text)


def get_embeddings(
    model: Optional[str] = None,
    batch_size: Optional[int] = None,
    max_tokens: Optional[int] = None,
    **kwargs,
) -> Embeddings:
    """
    è·å– Embedding æ¨¡å‹å®ä¾‹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ embedding_model
            - "text-embedding-3-small": å°å‹æ¨¡å‹,é€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½
            - "text-embedding-3-large": å¤§å‹æ¨¡å‹ï¼Œæ•ˆæœå¥½ï¼Œæˆæœ¬é«˜
            - "text-embedding-ada-002": æ—§ç‰ˆæ¨¡å‹ï¼ˆä¸æ¨èï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        max_tokens: å•ä¸ªæ–‡æœ¬çš„æœ€å¤§ token æ•°é™åˆ¶ï¼Œé»˜è®¤ 512
        **kwargs: å…¶ä»–ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
        
    Returns:
        Embeddings å®ä¾‹ï¼ˆåŒ…è£…äº† token é™åˆ¶å¤„ç†ï¼‰
        
    Example:
        >>> # ä½¿ç”¨é»˜è®¤é…ç½®
        >>> embeddings = get_embeddings()
        >>> 
        >>> # ä½¿ç”¨å¤§å‹æ¨¡å‹
        >>> embeddings = get_embeddings(model="text-embedding-3-large")
        >>> 
        >>> # åµŒå…¥å•ä¸ªæ–‡æœ¬
        >>> vector = embeddings.embed_query("ä½ å¥½ï¼Œä¸–ç•Œ")
        >>> print(f"å‘é‡ç»´åº¦: {len(vector)}")
        >>> 
        >>> # æ‰¹é‡åµŒå…¥
        >>> texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
        >>> vectors = embeddings.embed_documents(texts)
        >>> print(f"ç”Ÿæˆäº† {len(vectors)} ä¸ªå‘é‡")
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    model = model or settings.embedding_model
    batch_size = batch_size or settings.embedding_batch_size
    max_tokens = max_tokens or 512  # é»˜è®¤ 512 tokens é™åˆ¶
    
    logger.info(f"ğŸ”¢ åˆ›å»º Embedding æ¨¡å‹: {model}")
    logger.debug(f"   batch_size: {batch_size}")
    logger.debug(f"   max_tokens: {max_tokens}")
    
    try:
        # åˆ›å»º OpenAI Embeddings å®ä¾‹
        base_embeddings = OpenAIEmbeddings(
            model=model,
            api_key=settings.openai_api_key,
            base_url=settings.openai_api_base,
            # chunk_size å‚æ•°æ§åˆ¶æ‰¹å¤„ç†å¤§å°
            chunk_size=batch_size,
            **kwargs,
        )
        
        # ä½¿ç”¨ SafeOpenAIEmbeddings åŒ…è£…å™¨æ¥å¤„ç† token é™åˆ¶
        embeddings = SafeOpenAIEmbeddings(
            embeddings=base_embeddings,
            max_tokens=max_tokens,
        )
        
        logger.debug(f"âœ… Embedding æ¨¡å‹åˆ›å»ºæˆåŠŸï¼ˆå¸¦ token é™åˆ¶ä¿æŠ¤ï¼‰")
        return embeddings
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»º Embedding æ¨¡å‹å¤±è´¥: {e}")
        raise


def get_embedding_dimension(model: Optional[str] = None) -> int:
    """
    è·å– Embedding æ¨¡å‹çš„å‘é‡ç»´åº¦
    
    Args:
        model: æ¨¡å‹åç§°
        
    Returns:
        å‘é‡ç»´åº¦
        
    Example:
        >>> dim = get_embedding_dimension("text-embedding-3-small")
        >>> print(f"å‘é‡ç»´åº¦: {dim}")  # 1536
    """
    model = model or settings.embedding_model
    
    # OpenAI Embedding æ¨¡å‹çš„ç»´åº¦
    dimensions = {
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536,
    }
    
    if model not in dimensions:
        logger.warning(f"æœªçŸ¥çš„æ¨¡å‹ç»´åº¦: {model}ï¼Œè¿”å›é»˜è®¤å€¼ 1536")
        return 1536
    
    return dimensions[model]


def estimate_embedding_cost(
    num_tokens: int,
    model: Optional[str] = None,
) -> float:
    """
    ä¼°ç®— Embedding æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    
    Args:
        num_tokens: Token æ•°é‡
        model: æ¨¡å‹åç§°
        
    Returns:
        ä¼°ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
        
    Example:
        >>> # å‡è®¾æœ‰ 100,000 tokens
        >>> cost = estimate_embedding_cost(100000, "text-embedding-3-small")
        >>> print(f"ä¼°ç®—æˆæœ¬: ${cost:.4f}")
    """
    model = model or settings.embedding_model
    
    # OpenAI Embedding å®šä»·ï¼ˆæ¯ç™¾ä¸‡ tokens çš„ç¾å…ƒä»·æ ¼ï¼‰
    # å‚è€ƒ: https://openai.com/pricing
    pricing = {
        "text-embedding-3-small": 0.02,   # $0.02 / 1M tokens
        "text-embedding-3-large": 0.13,   # $0.13 / 1M tokens
        "text-embedding-ada-002": 0.10,   # $0.10 / 1M tokens
    }
    
    if model not in pricing:
        logger.warning(f"æœªçŸ¥çš„æ¨¡å‹å®šä»·: {model}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        price_per_million = 0.02
    else:
        price_per_million = pricing[model]
    
    # è®¡ç®—æˆæœ¬
    cost = (num_tokens / 1_000_000) * price_per_million
    
    logger.info(
        f"ğŸ’° Embedding æˆæœ¬ä¼°ç®—: "
        f"{num_tokens:,} tokens Ã— ${price_per_million}/M = ${cost:.4f}"
    )
    
    return cost


def test_embeddings(
    model: Optional[str] = None,
    test_text: str = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
) -> bool:
    """
    æµ‹è¯• Embedding æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
    
    Args:
        model: æ¨¡å‹åç§°
        test_text: æµ‹è¯•æ–‡æœ¬
        
    Returns:
        æ˜¯å¦æµ‹è¯•æˆåŠŸ
        
    Example:
        >>> if test_embeddings():
        ...     print("Embedding æ¨¡å‹å·¥ä½œæ­£å¸¸")
    """
    try:
        logger.info("ğŸ§ª æµ‹è¯• Embedding æ¨¡å‹...")
        
        embeddings = get_embeddings(model=model)
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬åµŒå…¥
        vector = embeddings.embed_query(test_text)
        logger.info(f"   å•æ–‡æœ¬åµŒå…¥: ç»´åº¦={len(vector)}")
        
        # æµ‹è¯•æ‰¹é‡åµŒå…¥
        texts = [test_text, test_text + " 2", test_text + " 3"]
        vectors = embeddings.embed_documents(texts)
        logger.info(f"   æ‰¹é‡åµŒå…¥: {len(vectors)} ä¸ªå‘é‡")
        
        logger.info("âœ… Embedding æ¨¡å‹æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Embedding æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False


# é¢„å®šä¹‰çš„ Embedding é…ç½®
EMBEDDING_CONFIGS = {
    "fast": {
        "model": "text-embedding-3-small",
        "description": "å¿«é€Ÿæ¨¡å‹ï¼Œé€‚åˆå¼€å‘å’Œæµ‹è¯•",
    },
    "quality": {
        "model": "text-embedding-3-large",
        "description": "é«˜è´¨é‡æ¨¡å‹ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ",
    },
    "legacy": {
        "model": "text-embedding-ada-002",
        "description": "æ—§ç‰ˆæ¨¡å‹ï¼ˆä¸æ¨èï¼‰",
    },
}


def get_embeddings_by_preset(
    preset: str = "fast",
    **kwargs,
) -> Embeddings:
    """
    æ ¹æ®é¢„è®¾é…ç½®è·å– Embedding æ¨¡å‹
    
    Args:
        preset: é¢„è®¾åç§°
            - "fast": å¿«é€Ÿæ¨¡å‹ï¼ˆtext-embedding-3-smallï¼‰
            - "quality": é«˜è´¨é‡æ¨¡å‹ï¼ˆtext-embedding-3-largeï¼‰
            - "legacy": æ—§ç‰ˆæ¨¡å‹ï¼ˆtext-embedding-ada-002ï¼‰
        **kwargs: è¦†ç›–é¢„è®¾çš„å‚æ•°
        
    Returns:
        Embeddings å®ä¾‹
        
    Raises:
        ValueError: å¦‚æœé¢„è®¾åç§°ä¸å­˜åœ¨
        
    Example:
        >>> # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
        >>> embeddings = get_embeddings_by_preset("fast")
        >>> 
        >>> # ä½¿ç”¨é«˜è´¨é‡æ¨¡å‹
        >>> embeddings = get_embeddings_by_preset("quality")
    """
    if preset not in EMBEDDING_CONFIGS:
        available = ", ".join(EMBEDDING_CONFIGS.keys())
        raise ValueError(
            f"æœªçŸ¥çš„é¢„è®¾: {preset}. å¯ç”¨é¢„è®¾: {available}"
        )
    
    config = EMBEDDING_CONFIGS[preset].copy()
    model = config.pop("model")
    config.pop("description", None)
    config.update(kwargs)
    
    logger.info(f"ğŸ“‹ ä½¿ç”¨é¢„è®¾ Embedding é…ç½®: {preset}")
    return get_embeddings(model=model, **config)

