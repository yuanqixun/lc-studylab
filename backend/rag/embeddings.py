"""
Embeddings æ¨¡å—

æä¾›ç»Ÿä¸€çš„ Embedding æ¨¡å‹æ¥å£ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ã€‚

æ”¯æŒçš„ Embedding æ¨¡å‹ï¼š
- OpenAI Embeddings (text-embedding-3-small, text-embedding-3-large)
- å¯æ‰©å±•æ”¯æŒå…¶ä»– embedding æ¨¡å‹

å‚è€ƒï¼š
- https://reference.langchain.com/python/langchain_core/embeddings/
- https://reference.langchain.com/python/langchain_openai/embeddings/
"""

from typing import Optional
from langchain_openai import OpenAIEmbeddings
from langchain_core.embeddings import Embeddings

from config import settings, get_logger

logger = get_logger(__name__)


def get_embeddings(
    model: Optional[str] = None,
    batch_size: Optional[int] = None,
    **kwargs,
) -> Embeddings:
    """
    è·å– Embedding æ¨¡å‹å®ä¾‹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ embedding_model
            - "text-embedding-3-small": å°å‹æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½
            - "text-embedding-3-large": å¤§å‹æ¨¡å‹ï¼Œæ•ˆæœå¥½ï¼Œæˆæœ¬é«˜
            - "text-embedding-ada-002": æ—§ç‰ˆæ¨¡å‹ï¼ˆä¸æ¨èï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        **kwargs: å…¶ä»–ä¼ é€’ç»™æ¨¡å‹çš„å‚æ•°
        
    Returns:
        Embeddings å®ä¾‹
        
    Example:
        >>> # ä½¿ç”¨é»˜è®¤é…ç½®
        >>> embeddings = get_embeddings()
        >>> 
        >>> # ä½¿ç”¨å¤§å‹æ¨¡å‹
        >>> embeddings = get_embeddings(model="text-embedding-3-large")
        >>> 
        >>> # åµŒå…¥å•ä¸ªæ–‡æœ¬
        >>> vector = embeddings.embed_query("ä½ å¥½ï¼Œä¸–ç•Œ")
        >>> print(f"å‘é‡ç»´åº¦: {len(vector)}")
        >>> 
        >>> # æ‰¹é‡åµŒå…¥
        >>> texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
        >>> vectors = embeddings.embed_documents(texts)
        >>> print(f"ç”Ÿæˆäº† {len(vectors)} ä¸ªå‘é‡")
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    model = model or settings.embedding_model
    batch_size = batch_size or settings.embedding_batch_size
    
    logger.info(f"ğŸ”¢ åˆ›å»º Embedding æ¨¡å‹: {model}")
    logger.debug(f"   batch_size: {batch_size}")
    
    try:
        # åˆ›å»º OpenAI Embeddings å®ä¾‹
        embeddings = OpenAIEmbeddings(
            model=model,
            api_key=settings.openai_api_key,
            base_url=settings.openai_api_base,
            # chunk_size å‚æ•°æ§åˆ¶æ‰¹å¤„ç†å¤§å°
            chunk_size=batch_size,
            **kwargs,
        )
        
        logger.debug(f"âœ… Embedding æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        return embeddings
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»º Embedding æ¨¡å‹å¤±è´¥: {e}")
        raise


def get_embedding_dimension(model: Optional[str] = None) -> int:
    """
    è·å– Embedding æ¨¡å‹çš„å‘é‡ç»´åº¦
    
    Args:
        model: æ¨¡å‹åç§°
        
    Returns:
        å‘é‡ç»´åº¦
        
    Example:
        >>> dim = get_embedding_dimension("text-embedding-3-small")
        >>> print(f"å‘é‡ç»´åº¦: {dim}")  # 1536
    """
    model = model or settings.embedding_model
    
    # OpenAI Embedding æ¨¡å‹çš„ç»´åº¦
    dimensions = {
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536,
    }
    
    if model not in dimensions:
        logger.warning(f"æœªçŸ¥çš„æ¨¡å‹ç»´åº¦: {model}ï¼Œè¿”å›é»˜è®¤å€¼ 1536")
        return 1536
    
    return dimensions[model]


def estimate_embedding_cost(
    num_tokens: int,
    model: Optional[str] = None,
) -> float:
    """
    ä¼°ç®— Embedding æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    
    Args:
        num_tokens: Token æ•°é‡
        model: æ¨¡å‹åç§°
        
    Returns:
        ä¼°ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
        
    Example:
        >>> # å‡è®¾æœ‰ 100,000 tokens
        >>> cost = estimate_embedding_cost(100000, "text-embedding-3-small")
        >>> print(f"ä¼°ç®—æˆæœ¬: ${cost:.4f}")
    """
    model = model or settings.embedding_model
    
    # OpenAI Embedding å®šä»·ï¼ˆæ¯ç™¾ä¸‡ tokens çš„ç¾å…ƒä»·æ ¼ï¼‰
    # å‚è€ƒ: https://openai.com/pricing
    pricing = {
        "text-embedding-3-small": 0.02,   # $0.02 / 1M tokens
        "text-embedding-3-large": 0.13,   # $0.13 / 1M tokens
        "text-embedding-ada-002": 0.10,   # $0.10 / 1M tokens
    }
    
    if model not in pricing:
        logger.warning(f"æœªçŸ¥çš„æ¨¡å‹å®šä»·: {model}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        price_per_million = 0.02
    else:
        price_per_million = pricing[model]
    
    # è®¡ç®—æˆæœ¬
    cost = (num_tokens / 1_000_000) * price_per_million
    
    logger.info(
        f"ğŸ’° Embedding æˆæœ¬ä¼°ç®—: "
        f"{num_tokens:,} tokens Ã— ${price_per_million}/M = ${cost:.4f}"
    )
    
    return cost


def test_embeddings(
    model: Optional[str] = None,
    test_text: str = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
) -> bool:
    """
    æµ‹è¯• Embedding æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
    
    Args:
        model: æ¨¡å‹åç§°
        test_text: æµ‹è¯•æ–‡æœ¬
        
    Returns:
        æ˜¯å¦æµ‹è¯•æˆåŠŸ
        
    Example:
        >>> if test_embeddings():
        ...     print("Embedding æ¨¡å‹å·¥ä½œæ­£å¸¸")
    """
    try:
        logger.info("ğŸ§ª æµ‹è¯• Embedding æ¨¡å‹...")
        
        embeddings = get_embeddings(model=model)
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬åµŒå…¥
        vector = embeddings.embed_query(test_text)
        logger.info(f"   å•æ–‡æœ¬åµŒå…¥: ç»´åº¦={len(vector)}")
        
        # æµ‹è¯•æ‰¹é‡åµŒå…¥
        texts = [test_text, test_text + " 2", test_text + " 3"]
        vectors = embeddings.embed_documents(texts)
        logger.info(f"   æ‰¹é‡åµŒå…¥: {len(vectors)} ä¸ªå‘é‡")
        
        logger.info("âœ… Embedding æ¨¡å‹æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Embedding æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False


# é¢„å®šä¹‰çš„ Embedding é…ç½®
EMBEDDING_CONFIGS = {
    "fast": {
        "model": "text-embedding-3-small",
        "description": "å¿«é€Ÿæ¨¡å‹ï¼Œé€‚åˆå¼€å‘å’Œæµ‹è¯•",
    },
    "quality": {
        "model": "text-embedding-3-large",
        "description": "é«˜è´¨é‡æ¨¡å‹ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ",
    },
    "legacy": {
        "model": "text-embedding-ada-002",
        "description": "æ—§ç‰ˆæ¨¡å‹ï¼ˆä¸æ¨èï¼‰",
    },
}


def get_embeddings_by_preset(
    preset: str = "fast",
    **kwargs,
) -> Embeddings:
    """
    æ ¹æ®é¢„è®¾é…ç½®è·å– Embedding æ¨¡å‹
    
    Args:
        preset: é¢„è®¾åç§°
            - "fast": å¿«é€Ÿæ¨¡å‹ï¼ˆtext-embedding-3-smallï¼‰
            - "quality": é«˜è´¨é‡æ¨¡å‹ï¼ˆtext-embedding-3-largeï¼‰
            - "legacy": æ—§ç‰ˆæ¨¡å‹ï¼ˆtext-embedding-ada-002ï¼‰
        **kwargs: è¦†ç›–é¢„è®¾çš„å‚æ•°
        
    Returns:
        Embeddings å®ä¾‹
        
    Raises:
        ValueError: å¦‚æœé¢„è®¾åç§°ä¸å­˜åœ¨
        
    Example:
        >>> # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
        >>> embeddings = get_embeddings_by_preset("fast")
        >>> 
        >>> # ä½¿ç”¨é«˜è´¨é‡æ¨¡å‹
        >>> embeddings = get_embeddings_by_preset("quality")
    """
    if preset not in EMBEDDING_CONFIGS:
        available = ", ".join(EMBEDDING_CONFIGS.keys())
        raise ValueError(
            f"æœªçŸ¥çš„é¢„è®¾: {preset}. å¯ç”¨é¢„è®¾: {available}"
        )
    
    config = EMBEDDING_CONFIGS[preset].copy()
    model = config.pop("model")
    config.pop("description", None)
    config.update(kwargs)
    
    logger.info(f"ğŸ“‹ ä½¿ç”¨é¢„è®¾ Embedding é…ç½®: {preset}")
    return get_embeddings(model=model, **config)

