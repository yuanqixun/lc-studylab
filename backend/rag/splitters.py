"""
æ–‡æœ¬åˆ†å—å™¨æ¨¡å—

æä¾›å¤šç§æ–‡æœ¬åˆ†å—ç­–ç•¥ï¼Œç”¨äºå°†é•¿æ–‡æ¡£åˆ†å‰²æˆé€‚åˆå‘é‡åŒ–çš„å°å—ã€‚

æ”¯æŒçš„åˆ†å—å™¨ï¼š
- RecursiveCharacterTextSplitter: é€’å½’å­—ç¬¦åˆ†å—ï¼ˆæ¨èï¼‰
- CharacterTextSplitter: ç®€å•å­—ç¬¦åˆ†å—
- MarkdownTextSplitter: Markdown ä¸“ç”¨åˆ†å—
- TokenTextSplitter: åŸºäº Token çš„åˆ†å—

å‚è€ƒï¼š
- https://reference.langchain.com/python/langchain_text_splitters/
"""

from typing import List, Optional, Literal
from langchain_core.documents import Document
from langchain_text_splitters import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    MarkdownTextSplitter,
    TokenTextSplitter,
)

from config import settings, get_logger

logger = get_logger(__name__)


# åˆ†å—å™¨ç±»å‹
SplitterType = Literal["recursive", "character", "markdown", "token"]


def get_text_splitter(
    splitter_type: SplitterType = "recursive",
    chunk_size: Optional[int] = None,
    chunk_overlap: Optional[int] = None,
    **kwargs,
):
    """
    è·å–æ–‡æœ¬åˆ†å—å™¨
    
    Args:
        splitter_type: åˆ†å—å™¨ç±»å‹
            - "recursive": é€’å½’å­—ç¬¦åˆ†å—ï¼ˆæ¨èï¼Œé€‚åˆå¤§å¤šæ•°æƒ…å†µï¼‰
            - "character": ç®€å•å­—ç¬¦åˆ†å—
            - "markdown": Markdown ä¸“ç”¨åˆ†å—
            - "token": åŸºäº Token çš„åˆ†å—
        chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°æˆ– token æ•°ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        chunk_overlap: åˆ†å—é‡å å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        **kwargs: å…¶ä»–ä¼ é€’ç»™åˆ†å—å™¨çš„å‚æ•°
        
    Returns:
        æ–‡æœ¬åˆ†å—å™¨å®ä¾‹
        
    Example:
        >>> # ä½¿ç”¨é»˜è®¤é…ç½®
        >>> splitter = get_text_splitter()
        >>> 
        >>> # è‡ªå®šä¹‰å‚æ•°
        >>> splitter = get_text_splitter(
        ...     splitter_type="recursive",
        ...     chunk_size=500,
        ...     chunk_overlap=50
        ... )
        >>> 
        >>> # Markdown ä¸“ç”¨
        >>> splitter = get_text_splitter(splitter_type="markdown")
    """
    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    chunk_size = chunk_size or settings.chunk_size
    chunk_overlap = chunk_overlap or settings.chunk_overlap
    
    logger.debug(
        f"åˆ›å»ºæ–‡æœ¬åˆ†å—å™¨: type={splitter_type}, "
        f"chunk_size={chunk_size}, chunk_overlap={chunk_overlap}"
    )
    
    if splitter_type == "recursive":
        # é€’å½’å­—ç¬¦åˆ†å—å™¨ï¼ˆæ¨èï¼‰
        # ä¼šå°è¯•æŒ‰ç…§ \n\n, \n, ç©ºæ ¼ç­‰åˆ†éš”ç¬¦é€’å½’åˆ†å‰²
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False,
            **kwargs,
        )
    
    elif splitter_type == "character":
        # ç®€å•å­—ç¬¦åˆ†å—å™¨
        return CharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separator="\n\n",  # é»˜è®¤æŒ‰æ®µè½åˆ†å‰²
            length_function=len,
            is_separator_regex=False,
            **kwargs,
        )
    
    elif splitter_type == "markdown":
        # Markdown ä¸“ç”¨åˆ†å—å™¨
        # ä¼šæŒ‰ç…§ Markdown çš„æ ‡é¢˜å±‚çº§è¿›è¡Œåˆ†å‰²
        return MarkdownTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            **kwargs,
        )
    
    elif splitter_type == "token":
        # åŸºäº Token çš„åˆ†å—å™¨
        # ä½¿ç”¨ tiktoken è®¡ç®— token æ•°é‡
        return TokenTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            **kwargs,
        )
    
    else:
        raise ValueError(
            f"ä¸æ”¯æŒçš„åˆ†å—å™¨ç±»å‹: {splitter_type}ã€‚"
            f"æ”¯æŒçš„ç±»å‹: recursive, character, markdown, token"
        )


def split_documents(
    documents: List[Document],
    splitter_type: SplitterType = "recursive",
    chunk_size: Optional[int] = None,
    chunk_overlap: Optional[int] = None,
    **kwargs,
) -> List[Document]:
    """
    åˆ†å—æ–‡æ¡£åˆ—è¡¨
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        splitter_type: åˆ†å—å™¨ç±»å‹
        chunk_size: åˆ†å—å¤§å°
        chunk_overlap: åˆ†å—é‡å å¤§å°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        
    Example:
        >>> from rag.loaders import load_document
        >>> 
        >>> # åŠ è½½æ–‡æ¡£
        >>> documents = load_document("document.pdf")
        >>> print(f"åŸå§‹æ–‡æ¡£: {len(documents)} ä¸ª")
        >>> 
        >>> # åˆ†å—
        >>> chunks = split_documents(documents)
        >>> print(f"åˆ†å—å: {len(chunks)} ä¸ª")
        >>> 
        >>> # æŸ¥çœ‹ç¬¬ä¸€ä¸ªå—
        >>> print(chunks[0].page_content[:200])
    """
    if not documents:
        logger.warning("æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œæ— éœ€åˆ†å—")
        return []
    
    logger.info(f"ğŸ“ å¼€å§‹åˆ†å—: {len(documents)} ä¸ªæ–‡æ¡£")
    
    # è·å–åˆ†å—å™¨
    splitter = get_text_splitter(
        splitter_type=splitter_type,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        **kwargs,
    )
    
    # æ‰§è¡Œåˆ†å—
    try:
        chunks = splitter.split_documents(documents)
        
        logger.info(f"âœ… åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(len(chunk.page_content) for chunk in chunks)
        avg_chars = total_chars / len(chunks) if chunks else 0
        
        logger.info(f"   å¹³å‡å—å¤§å°: {avg_chars:.0f} å­—ç¬¦")
        logger.info(f"   æ€»å­—ç¬¦æ•°: {total_chars}")
        
        return chunks
        
    except Exception as e:
        logger.error(f"âŒ åˆ†å—å¤±è´¥: {e}")
        raise


def split_text(
    text: str,
    splitter_type: SplitterType = "recursive",
    chunk_size: Optional[int] = None,
    chunk_overlap: Optional[int] = None,
    metadata: Optional[dict] = None,
    **kwargs,
) -> List[Document]:
    """
    åˆ†å—çº¯æ–‡æœ¬
    
    Args:
        text: è¦åˆ†å—çš„æ–‡æœ¬
        splitter_type: åˆ†å—å™¨ç±»å‹
        chunk_size: åˆ†å—å¤§å°
        chunk_overlap: åˆ†å—é‡å å¤§å°
        metadata: è¦æ·»åŠ åˆ°æ‰€æœ‰å—çš„å…ƒæ•°æ®
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        
    Example:
        >>> text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬..." * 1000
        >>> chunks = split_text(
        ...     text,
        ...     chunk_size=500,
        ...     metadata={"source": "manual_input"}
        ... )
    """
    if not text:
        logger.warning("æ–‡æœ¬ä¸ºç©ºï¼Œæ— éœ€åˆ†å—")
        return []
    
    logger.info(f"ğŸ“ å¼€å§‹åˆ†å—æ–‡æœ¬: {len(text)} å­—ç¬¦")
    
    # è·å–åˆ†å—å™¨
    splitter = get_text_splitter(
        splitter_type=splitter_type,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        **kwargs,
    )
    
    # æ‰§è¡Œåˆ†å—
    try:
        # ä½¿ç”¨ create_documents æ–¹æ³•ï¼Œå¯ä»¥æ·»åŠ å…ƒæ•°æ®
        metadatas = [metadata] if metadata else None
        chunks = splitter.create_documents(
            texts=[text],
            metadatas=metadatas,
        )
        
        logger.info(f"âœ… åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        return chunks
        
    except Exception as e:
        logger.error(f"âŒ åˆ†å—å¤±è´¥: {e}")
        raise


def get_optimal_chunk_size(
    document_type: str = "general",
) -> tuple[int, int]:
    """
    æ ¹æ®æ–‡æ¡£ç±»å‹è·å–æ¨èçš„åˆ†å—å‚æ•°
    
    Args:
        document_type: æ–‡æ¡£ç±»å‹
            - "general": é€šç”¨æ–‡æ¡£ï¼ˆé»˜è®¤ï¼‰
            - "code": ä»£ç æ–‡æ¡£
            - "markdown": Markdown æ–‡æ¡£
            - "academic": å­¦æœ¯è®ºæ–‡
            - "chat": å¯¹è¯è®°å½•
            
    Returns:
        (chunk_size, chunk_overlap) å…ƒç»„
        
    Example:
        >>> chunk_size, overlap = get_optimal_chunk_size("code")
        >>> splitter = get_text_splitter(
        ...     chunk_size=chunk_size,
        ...     chunk_overlap=overlap
        ... )
    """
    # ä¸åŒç±»å‹æ–‡æ¡£çš„æ¨èå‚æ•°
    recommendations = {
        "general": (1000, 200),      # é€šç”¨æ–‡æ¡£
        "code": (1500, 300),          # ä»£ç éœ€è¦æ›´å¤§çš„ä¸Šä¸‹æ–‡
        "markdown": (800, 150),       # Markdown é€šå¸¸ç»“æ„æ¸…æ™°
        "academic": (1200, 250),      # å­¦æœ¯è®ºæ–‡éœ€è¦ä¿æŒä¸Šä¸‹æ–‡
        "chat": (500, 50),            # å¯¹è¯è®°å½•å¯ä»¥æ›´å°
    }
    
    if document_type not in recommendations:
        logger.warning(
            f"æœªçŸ¥çš„æ–‡æ¡£ç±»å‹: {document_type}ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°"
        )
        return recommendations["general"]
    
    chunk_size, overlap = recommendations[document_type]
    logger.info(
        f"ğŸ“Š æ¨èçš„åˆ†å—å‚æ•° ({document_type}): "
        f"chunk_size={chunk_size}, overlap={overlap}"
    )
    
    return chunk_size, overlap


def analyze_chunks(chunks: List[Document]) -> dict:
    """
    åˆ†æåˆ†å—ç»“æœçš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        chunks: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        
    Returns:
        åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        
    Example:
        >>> chunks = split_documents(documents)
        >>> stats = analyze_chunks(chunks)
        >>> print(f"å¹³å‡å—å¤§å°: {stats['avg_chunk_size']}")
    """
    if not chunks:
        return {
            "total_chunks": 0,
            "total_chars": 0,
            "avg_chunk_size": 0,
            "min_chunk_size": 0,
            "max_chunk_size": 0,
        }
    
    chunk_sizes = [len(chunk.page_content) for chunk in chunks]
    total_chars = sum(chunk_sizes)
    
    stats = {
        "total_chunks": len(chunks),
        "total_chars": total_chars,
        "avg_chunk_size": total_chars / len(chunks),
        "min_chunk_size": min(chunk_sizes),
        "max_chunk_size": max(chunk_sizes),
    }
    
    logger.info("ğŸ“Š åˆ†å—ç»Ÿè®¡:")
    logger.info(f"   æ€»å—æ•°: {stats['total_chunks']}")
    logger.info(f"   æ€»å­—ç¬¦æ•°: {stats['total_chars']}")
    logger.info(f"   å¹³å‡å¤§å°: {stats['avg_chunk_size']:.0f} å­—ç¬¦")
    logger.info(f"   æœ€å°å—: {stats['min_chunk_size']} å­—ç¬¦")
    logger.info(f"   æœ€å¤§å—: {stats['max_chunk_size']} å­—ç¬¦")
    
    return stats

