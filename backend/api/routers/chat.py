"""
èŠå¤© API è·¯ç”±
æä¾› /chat æ¥å£ï¼Œæ”¯æŒæµå¼å’Œéæµå¼å¯¹è¯

è¿™æ˜¯ç¬¬ 1 é˜¶æ®µçš„ API æ¥å£ï¼Œå®ç°ï¼š
1. POST /chat - éæµå¼å¯¹è¯
2. POST /chat/stream - æµå¼å¯¹è¯ï¼ˆSSEï¼‰
3. æ”¯æŒå¯¹è¯å†å²ç®¡ç†
4. æ”¯æŒä¸åŒçš„ Agent æ¨¡å¼
"""

from typing import List, Optional, Dict, Any
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
import json
import asyncio
import time

from agents import create_base_agent
from core.tools import BASIC_TOOLS, WEB_SEARCH_TOOLS, WEATHER_TOOLS
from deep_research import create_deep_research_agent
from config import settings, get_logger

logger = get_logger(__name__)

# åˆ›å»ºè·¯ç”±å™¨
router = APIRouter(prefix="/chat", tags=["chat"])


# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================

class Message(BaseModel):
    """æ¶ˆæ¯æ¨¡å‹"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²ï¼šuser/assistant/system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    message: str = Field(..., description="ç”¨æˆ·æ¶ˆæ¯", min_length=1)
    chat_history: Optional[List[Message]] = Field(
        default=None,
        description="å¯¹è¯å†å²"
    )
    mode: str = Field(
        default="default",
        description="Agent æ¨¡å¼ï¼šdefault/coding/research/concise/detailed"
    )
    use_tools: bool = Field(
        default=True,
        description="æ˜¯å¦å¯ç”¨å·¥å…·"
    )
    use_advanced_tools: bool = Field(
        default=False,
        description="æ˜¯å¦å¯ç”¨é«˜çº§å·¥å…·ï¼ˆéœ€è¦ API Keyï¼‰"
    )
    streaming: bool = Field(
        default=False,
        description="æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆæ­¤å­—æ®µåœ¨éæµå¼æ¥å£ä¸­æ— æ•ˆï¼‰"
    )


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    message: str = Field(..., description="AI å›å¤")
    mode: str = Field(..., description="ä½¿ç”¨çš„ Agent æ¨¡å¼")
    tools_used: List[str] = Field(default_factory=list, description="ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨")
    success: bool = Field(default=True, description="æ˜¯å¦æˆåŠŸ")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")


# ==================== è¾…åŠ©å‡½æ•° ====================

DEEP_RESEARCH_KEYWORDS = [
    "æ·±åº¦", "ç ”ç©¶", "è¶‹åŠ¿", "å¯¹æ¯”", "åˆ†æ", "æŠ¥å‘Š", "æ€»ç»“",
    "æœªæ¥", "å½±å“", "å¸‚åœº", "å‘å±•", "æœºåˆ¶", "åŸç†", "workflow",
    "architecture", "best practice", "æœ€ä½³å®è·µ", "è¯¦è§£", "è¯¦è¿°",
    "react", "hooks", "langchain", "ai", "å¤§æ¨¡å‹", "æœºå™¨å­¦ä¹ ",
    "ç¼–è¯‘å™¨", "æ¶æ„", "æ¨¡å¼", "best practice", "æœ€ä½³ å®è·µ",
    "ä½¿ç”¨æ–¹æ³•", "ä»‹ç»", "explain", "how to", "åŸç†", "æ·±å…¥",
]


def should_use_deep_research(message: str) -> bool:
    """
    ç®€å•åˆ¤æ–­é—®é¢˜æ˜¯å¦éœ€è¦æ·±åº¦ç ”ç©¶
    """
    if not message:
        return False
    lower = message.lower()
    if any(keyword in message for keyword in DEEP_RESEARCH_KEYWORDS):
        return True
    if len(message.strip()) >= 80:
        return True
    if lower.count("?") + message.count("ï¼Ÿ") >= 2:
        return True
    return False


async def run_deep_research_task(query: str) -> Dict[str, Any]:
    """
    åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæ·±åº¦ç ”ç©¶ä»»åŠ¡ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
    """
    loop = asyncio.get_running_loop()
    thread_id = f"deep_{int(time.time() * 1000)}"

    def _task():
        agent = create_deep_research_agent(
            thread_id=thread_id,
            enable_web_search=True,
            enable_doc_analysis=False,
        )
        return agent.research(query)

    return await loop.run_in_executor(None, _task)


def get_tools_for_request(use_tools: bool, use_advanced_tools: bool) -> List:
    """
    æ ¹æ®è¯·æ±‚å‚æ•°è·å–å·¥å…·åˆ—è¡¨
    
    è§„åˆ™ï¼š
    1. å¦‚æœç”¨æˆ·å…³é—­ use_toolsï¼Œåˆ™ä¸åŠ è½½ä»»ä½•å·¥å…·
    2. å¤©æ°”å·¥å…·åªè¦é…ç½®äº† AMAP_KEYï¼Œå°±é»˜è®¤æä¾›ï¼ˆå¸¸è§é—®ç­”åœºæ™¯ï¼‰
    """
    if not use_tools:
        return []
    
    tools: List = list(BASIC_TOOLS)
    
    # è‡ªåŠ¨æ³¨å…¥å¤©æ°”å·¥å…·ï¼ˆå‰æï¼šé…ç½®äº†é«˜å¾· API Keyï¼‰
    if settings.amap_key:
        for tool in WEATHER_TOOLS:
            if tool not in tools:
                tools.append(tool)
    else:
        logger.debug("ğŸŒ¤ï¸ æœªé…ç½® AMAP_KEYï¼Œå¤©æ°”å·¥å…·ä¸å¯ç”¨")
    
    # é»˜è®¤å¯ç”¨ Tavily æœç´¢ï¼ˆåªè¦é…ç½®äº† API Keyï¼‰
    if settings.tavily_api_key:
        for tool in WEB_SEARCH_TOOLS:
            if tool not in tools:
                tools.append(tool)
    else:
        logger.debug("ğŸŒ æœªé…ç½® Tavily API Keyï¼Œç½‘ç»œæœç´¢å·¥å…·ä¸å¯ç”¨")
    
    return tools


def convert_chat_history(messages: Optional[List[Message]]) -> List:
    """
    å°† API çš„æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º LangChain çš„æ¶ˆæ¯æ ¼å¼
    
    Args:
        messages: API æ¶ˆæ¯åˆ—è¡¨
        
    Returns:
        LangChain æ¶ˆæ¯åˆ—è¡¨
    """
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
    
    if not messages:
        return []
    
    langchain_messages = []
    for msg in messages:
        if msg.role == "user":
            langchain_messages.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            langchain_messages.append(AIMessage(content=msg.content))
        elif msg.role == "system":
            langchain_messages.append(SystemMessage(content=msg.content))
    
    return langchain_messages


# ==================== API ç«¯ç‚¹ ====================

@router.post("/", response_model=ChatResponse)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    éæµå¼èŠå¤©æ¥å£
    
    æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯ï¼Œè¿”å› AI çš„å®Œæ•´å›å¤ã€‚
    é€‚åˆéœ€è¦ä¸€æ¬¡æ€§è·å–å®Œæ•´å“åº”çš„åœºæ™¯ã€‚
    
    Args:
        request: èŠå¤©è¯·æ±‚
        
    Returns:
        èŠå¤©å“åº”
        
    Example:
        ```bash
        curl -X POST "http://localhost:8000/chat" \\
          -H "Content-Type: application/json" \\
          -d '{
            "message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
            "mode": "default",
            "use_tools": true
          }'
        ```
    """
    logger.info(f"ğŸ“¨ æ”¶åˆ°èŠå¤©è¯·æ±‚: {request.message[:50]}...")
    logger.debug(f"   æ¨¡å¼: {request.mode}, å·¥å…·: {request.use_tools}")
    
    try:
        # è·å–å·¥å…·åˆ—è¡¨
        tools = get_tools_for_request(request.use_tools, request.use_advanced_tools)
        
        # åˆ›å»º Agent
        agent = create_base_agent(
            tools=tools,
            prompt_mode=request.mode,
            # streaming=False,  # éæµå¼æ¥å£
        )
        
        # è½¬æ¢å¯¹è¯å†å²
        chat_history = convert_chat_history(request.chat_history)
        
        # é…ç½® - å¢åŠ é€’å½’é™åˆ¶ä»¥æ”¯æŒå¤æ‚çš„å·¥å…·è°ƒç”¨é“¾
        config = {
            "recursion_limit": 50,  # å¢åŠ é€’å½’é™åˆ¶ï¼ˆé»˜è®¤ 25ï¼‰
        }
        
        # è°ƒç”¨ Agent
        response = await agent.ainvoke(
            input_text=request.message,
            chat_history=chat_history,
            config=config,
        )
        def _needs_completion(text: str) -> bool:
            if not text:
                return True
            t = text.strip()
            if len(t) < 30:
                return True
            if not any(t.endswith(p) for p in ["ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]):
                return True
            return False
        if _needs_completion(response):
            from core.models import get_chat_model
            model = get_chat_model()
            prompt = (
                f"ç”¨æˆ·é—®é¢˜ï¼š{request.message}\n\n"
                f"å½“å‰å›å¤ï¼ˆä¸å®Œæ•´ï¼‰ï¼š{response}\n\n"
                "è¯·ç»§ç»­å¹¶å®Œæ•´å›ç­”ä¸Šè¿°é—®é¢˜ï¼Œè¡¥å……å¿…è¦çš„è§£é‡Šæˆ–ä¾‹å­ï¼Œæœ€åç»™å‡ºä¸€å¥ç®€æ˜ç»“è®ºã€‚"
            )
            completion = await model.ainvoke([{ "role": "user", "content": prompt }])
            if getattr(completion, "content", None):
                response = completion.content
        
        # æ„å»ºå“åº”
        tool_names = [tool.name for tool in tools]
        
        logger.info(f"âœ… èŠå¤©è¯·æ±‚å¤„ç†å®Œæˆï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        
        return ChatResponse(
            message=response,
            mode=request.mode,
            tools_used=tool_names,
            success=True,
        )
        
    except Exception as e:
        error_msg = f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        return ChatResponse(
            message="æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚",
            mode=request.mode,
            tools_used=[],
            success=False,
            error=str(e),
        )


@router.post("/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼èŠå¤©æ¥å£ï¼ˆSSE - Server-Sent Eventsï¼‰- å¢å¼ºç‰ˆ
    
    æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯ï¼Œä»¥æµå¼æ–¹å¼è¿”å› AI çš„å›å¤ã€‚
    é€‚åˆéœ€è¦å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹çš„åœºæ™¯ã€‚
    
    å¢å¼ºåŠŸèƒ½:
    - æ”¯æŒå·¥å…·è°ƒç”¨è¯¦æƒ…è¾“å‡º
    - æ”¯æŒæ¨ç†è¿‡ç¨‹è¾“å‡º
    - æ”¯æŒ token ä½¿ç”¨ç»Ÿè®¡
    - æ”¯æŒæ¥æºå¼•ç”¨è¾“å‡º
    - æ”¯æŒè®¡åˆ’å’Œä»»åŠ¡è¾“å‡º
    
    Args:
        request: èŠå¤©è¯·æ±‚
        
    Returns:
        SSE æµå¼å“åº”
        
    å“åº”æ ¼å¼ï¼ˆSSEï¼‰:
        ```
        data: {"type": "start", "message": "å¼€å§‹ç”Ÿæˆ..."}
        data: {"type": "chunk", "content": "æ–‡æœ¬å†…å®¹"}
        data: {"type": "tool", "data": {...}}
        data: {"type": "reasoning", "data": {...}}
        data: {"type": "source", "data": {...}}
        data: {"type": "context", "data": {...}}
        data: {"type": "end", "message": "ç”Ÿæˆå®Œæˆ"}
        ```
    """
    logger.info(f"ğŸŒŠ æ”¶åˆ°æµå¼èŠå¤©è¯·æ±‚: {request.message[:50]}...")
    
    async def generate():
        """SSE ç”Ÿæˆå™¨å‡½æ•° - å¢å¼ºç‰ˆ"""
        from core.usage_tracker import create_usage_tracker
        from core.extractors import MessageExtractor
        from langchain_core.messages import AIMessage, ToolMessage
        
        try:
            # å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'message': 'å¼€å§‹ç”Ÿæˆ...'}, ensure_ascii=False)}\n\n"
            
            # åˆ›å»º usage tracker
            usage_tracker = create_usage_tracker()
            
            # åˆ›å»ºæ¶ˆæ¯æå–å™¨
            extractor = MessageExtractor()

            # å¦‚æœé—®é¢˜éœ€è¦æ·±åº¦ç ”ç©¶ï¼Œä¼˜å…ˆèµ° DeepResearch æµç¨‹
            if should_use_deep_research(request.message):
                logger.info("ğŸ§  è§¦å‘æ·±åº¦ç ”ç©¶æµç¨‹ï¼Œäº¤ç»™ DeepResearchAgent å¤„ç†")
                
                reasoning_event = {
                    "type": "reasoning",
                    "data": {
                        "content": "é—®é¢˜è¾ƒä¸ºå¤æ‚ï¼Œæ­£åœ¨è°ƒåº¦æ·±åº¦ç ”ç©¶å·¥ä½œæµå¹¶æ‰§è¡Œç½‘ç»œæœç´¢...",
                        "duration": 0,
                    },
                }
                yield f"data: {json.dumps(reasoning_event, ensure_ascii=False)}\n\n"
                
                deep_result = await run_deep_research_task(request.message)
                final_report = deep_result.get("final_report") or deep_result.get("error")
                if not final_report:
                    final_report = (
                        "æ·±åº¦ç ”ç©¶å·²å®Œæˆï¼Œä½†æœªç”Ÿæˆå¯ç”¨æŠ¥å‘Šã€‚"
                        " è¯·ç¨åé‡è¯•æˆ–è°ƒæ•´é—®é¢˜è¡¨è¿°ã€‚"
                    )
                
                chunk_data = {
                    "type": "chunk",
                    "content": final_report,
                }
                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                
                # å‘é€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ·±åº¦ç ”ç©¶æ¨¡å¼ä¸‹ token ç»Ÿè®¡å¯èƒ½ä¸ºé›¶ï¼‰
                context_info = usage_tracker.get_usage_info()
                yield f"data: {json.dumps({'type': 'context', 'data': context_info}, ensure_ascii=False)}\n\n"
                
                # ç»“æŸäº‹ä»¶
                yield f"data: {json.dumps({'type': 'end', 'message': 'ç”Ÿæˆå®Œæˆ'}, ensure_ascii=False)}\n\n"
                
                usage_tracker.log_summary()
                logger.info("âœ… æ·±åº¦ç ”ç©¶æµç¨‹å®Œæˆ")
                return
            
            # è·å–å·¥å…·åˆ—è¡¨
            tools = get_tools_for_request(request.use_tools, request.use_advanced_tools)
            tool_names = [tool.name for tool in tools]
            weather_tool_names = {tool.name for tool in WEATHER_TOOLS}
            
            # åˆ›å»º Agentï¼ˆå¯ç”¨æµå¼ï¼‰
            agent = create_base_agent(
                tools=tools,
                prompt_mode=request.mode,
            )
            
            # è½¬æ¢å¯¹è¯å†å²
            chat_history = convert_chat_history(request.chat_history)
            
            # å‡†å¤‡è¾“å…¥
            messages = []
            if chat_history:
                messages.extend(chat_history)
            
            from langchain_core.messages import HumanMessage
            messages.append(HumanMessage(content=request.message))
            
            graph_input = {"messages": messages}
            
            # è¿½è¸ªå·¥å…·è°ƒç”¨
            tool_calls_map = {}
            current_message_content = ""
            last_ai_message = None  # ä¿å­˜æœ€åä¸€æ¡ AI æ¶ˆæ¯
            all_messages = []  # ä¿å­˜æ‰€æœ‰æ¶ˆæ¯ï¼Œç”¨äºæœ€ç»ˆæå–
            tool_call_count = {}
            prefer_tool_result = False
            
            # é…ç½® - å¢åŠ é€’å½’é™åˆ¶ä»¥æ”¯æŒå¤æ‚çš„å·¥å…·è°ƒç”¨é“¾
            config = {
                "recursion_limit": 50,  # å¢åŠ é€’å½’é™åˆ¶ï¼ˆé»˜è®¤ 25ï¼‰
            }
            
            # ä½¿ç”¨ graph.astream è·å–æ›´è¯¦ç»†çš„è¾“å‡º
            async for chunk in agent.graph.astream(graph_input, config=config, stream_mode="messages"):
                if isinstance(chunk, tuple) and len(chunk) == 2:
                    message, metadata = chunk
                else:
                    message = chunk
                    metadata = {}
                
                # æ›´æ–° token ä½¿ç”¨
                if metadata:
                    usage_tracker.update_from_metadata(metadata)
                
                # ä¿å­˜æ‰€æœ‰æ¶ˆæ¯
                all_messages.append(message)
                
                # å¤„ç† AI æ¶ˆæ¯
                if isinstance(message, AIMessage):
                    # ä¿å­˜æœ€åä¸€æ¡ AI æ¶ˆæ¯
                    last_ai_message = message
                    
                    # æå–å¹¶å‘é€å·¥å…·è°ƒç”¨
                    tool_calls = getattr(message, "tool_calls", [])
                    if tool_calls:
                        for tool_call in tool_calls:
                            tool_id = tool_call.get("id", "")
                            tool_name = tool_call.get("name", "")
                            
                            # è¿½è¸ªå·¥å…·è°ƒç”¨æ¬¡æ•°
                            if tool_name not in tool_call_count:
                                tool_call_count[tool_name] = 0
                            tool_call_count[tool_name] += 1
                            
                            # å¦‚æœåŒä¸€ä¸ªå·¥å…·è¢«è°ƒç”¨è¶…è¿‡ 3 æ¬¡ï¼Œè®°å½•è­¦å‘Š
                            if tool_call_count[tool_name] > 3:
                                logger.warning(f"âš ï¸ å·¥å…· {tool_name} è¢«è°ƒç”¨äº† {tool_call_count[tool_name]} æ¬¡ï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯è°ƒç”¨")
                            
                            tool_info = {
                                "id": tool_id,
                                "name": tool_name,
                                "type": f"tool-call-{tool_name}",
                                "state": "input-available",
                                "parameters": tool_call.get("args", {}),
                                "result": None,
                                "error": None,
                            }
                            tool_calls_map[tool_id] = tool_info
                            
                            # å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶
                            yield f"data: {json.dumps({'type': 'tool', 'data': tool_info}, ensure_ascii=False)}\n\n"
                    
                    if message.content and not tool_calls and not prefer_tool_result:
                        def _lcp_len(a: str, b: str) -> int:
                            i = 0
                            for ca, cb in zip(a, b):
                                if ca != cb:
                                    break
                                i += 1
                            return i
                        lcp = _lcp_len(current_message_content, message.content)
                        if lcp < len(message.content):
                            new_content = message.content[lcp:]
                            current_message_content = message.content
                            chunk_data = {
                                "type": "chunk",
                                "content": new_content,
                            }
                            yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                    
                    # æå–æ¨ç†è¿‡ç¨‹
                    from core.extractors import extract_reasoning
                    reasoning = extract_reasoning(message)
                    if reasoning:
                        yield f"data: {json.dumps({'type': 'reasoning', 'data': reasoning}, ensure_ascii=False)}\n\n"
                
                # å¤„ç†å·¥å…·ç»“æœ
                elif isinstance(message, ToolMessage):
                    tool_call_id = getattr(message, "tool_call_id", "")
                    is_error = getattr(message, "status", None) == "error"
                    
                    if tool_call_id in tool_calls_map:
                        tool_info = tool_calls_map[tool_call_id]
                        tool_info["state"] = "output-error" if is_error else "output-available"
                        tool_info["result"] = None if is_error else message.content
                        tool_info["error"] = message.content if is_error else None
                        
                        # å‘é€å·¥å…·ç»“æœæ›´æ–°
                        yield f"data: {json.dumps({'type': 'tool_result', 'data': tool_info}, ensure_ascii=False)}\n\n"
                        
                        # é’ˆå¯¹å¤©æ°”ç±»å·¥å…·ï¼Œç›´æ¥å°†ç»“æœä½œä¸ºåŠ©æ‰‹å›å¤æ¨é€ï¼Œé¿å…ç­‰å¾…æ¨¡å‹å†æ¬¡æ€»ç»“
                        if (not is_error 
                                and tool_info.get("name") in weather_tool_names
                                and tool_info.get("result")
                                and not tool_info.get("delivered")):
                            weather_result = tool_info["result"]
                            chunk_data = {
                                "type": "chunk",
                                "content": weather_result,
                            }
                            yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                            current_message_content += weather_result
                            
                            # å°†è¯¥ç»“æœä½œä¸º AIMessage ä¿å­˜ï¼Œç¡®ä¿å†å²è®°å½•å®Œæ•´
                            ai_message = AIMessage(content=weather_result)
                            all_messages.append(ai_message)
                            last_ai_message = ai_message
                            
                            # é˜²æ­¢é‡å¤å‘é€
                            tool_info["delivered"] = True
                            prefer_tool_result = True
                
                # å°å»¶è¿Ÿ
                await asyncio.sleep(0.01)
            
            # ä»æ‰€æœ‰æ¶ˆæ¯ä¸­æå–æœ€ç»ˆå›å¤
            # ä¼˜å…ˆæŸ¥æ‰¾æœ€åä¸€æ¡æœ‰å†…å®¹çš„ AI æ¶ˆæ¯
            final_ai_message = None
            for msg in reversed(all_messages):
                if isinstance(msg, AIMessage) and msg.content and msg.content.strip():
                    final_ai_message = msg
                    break
            
            # å¦‚æœæ‰¾åˆ°äº†æœ€ç»ˆ AI æ¶ˆæ¯ï¼Œå‘é€å…¶å†…å®¹
            if final_ai_message and final_ai_message.content:
                final_content = final_ai_message.content
                # å¦‚æœè¿˜æœ‰æœªå‘é€çš„å†…å®¹ï¼Œå‘é€å®ƒ
                if len(final_content) > len(current_message_content):
                    remaining_content = final_content[len(current_message_content):]
                    if remaining_content:
                        chunk_data = {
                            "type": "chunk",
                            "content": remaining_content,
                        }
                        yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                        current_message_content = final_content
            
            # å¦‚æœæœ€ç»ˆæ¶ˆæ¯ä¸ºç©ºæˆ–å†…å®¹å¾ˆå°‘ï¼Œä½†æœ‰å·¥å…·è°ƒç”¨ç»“æœï¼Œä½¿ç”¨å·¥å…·ç»“æœä½œä¸ºå›å¤
            if (not final_ai_message or not final_ai_message.content or len(final_ai_message.content.strip()) < 10) and tool_calls_map:
                # æŸ¥æ‰¾å¤©æ°”å·¥å…·çš„ç»“æœï¼ˆä¼˜å…ˆï¼‰
                weather_tools = ["get_daily_weather", "get_weather_forecast", "get_weather"]
                for tool_name in weather_tools:
                    for tool_info in tool_calls_map.values():
                        if (tool_info.get("name") == tool_name and 
                            tool_info.get("state") == "output-available" and 
                            tool_info.get("result")):
                            result_content = tool_info.get("result", "")
                            if result_content and result_content not in current_message_content:
                                # å‘é€å·¥å…·ç»“æœä½œä¸ºæœ€ç»ˆå›å¤
                                chunk_data = {
                                    "type": "chunk",
                                    "content": result_content,
                                }
                                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                                logger.info(f"âœ… ä½¿ç”¨å·¥å…· {tool_name} çš„ç»“æœä½œä¸ºæœ€ç»ˆå›å¤")
                                break
                    else:
                        continue
                    break
                else:
                    # å¦‚æœæ²¡æœ‰å¤©æ°”å·¥å…·ç»“æœï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸçš„å·¥å…·ç»“æœ
                    for tool_info in tool_calls_map.values():
                        if (tool_info.get("state") == "output-available" and 
                            tool_info.get("result") and
                            tool_info.get("result") not in current_message_content):
                            result_content = tool_info.get("result", "")
                            if result_content:
                                chunk_data = {
                                    "type": "chunk",
                                    "content": result_content,
                                }
                                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                        logger.info(f"âœ… ä½¿ç”¨å·¥å…· {tool_info.get('name')} çš„ç»“æœä½œä¸ºæœ€ç»ˆå›å¤")
                        break

            def _needs_completion(text: str) -> bool:
                if not text:
                    return True
                t = text.strip()
                if len(t) < 30:
                    return True
                if not any(t.endswith(p) for p in ["ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]):
                    return True
                return False

            if not prefer_tool_result and _needs_completion(current_message_content):
                from core.models import get_chat_model
                model = get_chat_model()
                prompt = (
                    f"ç”¨æˆ·é—®é¢˜ï¼š{request.message}\n\n"
                    f"å½“å‰å›å¤ï¼ˆä¸å®Œæ•´ï¼‰ï¼š{current_message_content}\n\n"
                    "è¯·ç»§ç»­å¹¶å®Œæ•´å›ç­”ä¸Šè¿°é—®é¢˜ï¼Œè¡¥å……å¿…è¦çš„è§£é‡Šæˆ–ä¾‹å­ï¼Œæœ€åç»™å‡ºä¸€å¥ç®€æ˜ç»“è®ºã€‚"
                )
                try:
                    completion = await model.ainvoke([{ "role": "user", "content": prompt }])
                    extra = getattr(completion, "content", "")
                    if extra:
                        chunk_data = {
                            "type": "chunk",
                            "content": extra,
                        }
                        yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
                        current_message_content += extra
                except Exception:
                    pass
            
            # ç”ŸæˆåŠ¨æ€å»ºè®®ï¼ˆåŸºäºç”¨æˆ·é—®é¢˜ä¸æœ€ç»ˆåŠ©æ‰‹å›å¤ï¼‰
            try:
                from core.models import get_chat_model
                model = get_chat_model()
                suggestions_prompt = (
                    "ä½ æ˜¯ä¸€ä¸ªè¾…åŠ©å¯¹è¯çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·é—®é¢˜å’Œæœ€ç»ˆå›å¤ï¼Œç”Ÿæˆ4æ¡ç®€æ´ã€ç›¸å…³ã€å¯ç‚¹å‡»çš„åç»­é—®é¢˜å»ºè®®ã€‚\n"
                    "ç”¨JSONæ•°ç»„è¿”å›ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸è¶…è¿‡30å­—çš„ä¸­æ–‡å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ç¼–å·æˆ–å¤šä½™æ–‡æœ¬ã€‚\n\n"
                    f"ç”¨æˆ·é—®é¢˜ï¼š{request.message}\n\n"
                    f"æœ€ç»ˆå›å¤ï¼š{current_message_content}"
                )
                completion = await model.ainvoke([{ "role": "user", "content": suggestions_prompt }])
                raw = getattr(completion, "content", "")
                suggestions: list[str] = []
                try:
                    parsed = json.loads(raw)
                    if isinstance(parsed, list):
                        suggestions = [str(x) for x in parsed if isinstance(x, (str, int, float))]
                        suggestions = [s for s in suggestions if s.strip()][:4]
                except Exception:
                    # å°è¯•æå–JSONç‰‡æ®µ
                    import re
                    m = re.search(r"\[.*\]", raw, re.DOTALL)
                    if m:
                        try:
                            parsed2 = json.loads(m.group(0))
                            if isinstance(parsed2, list):
                                suggestions = [str(x) for x in parsed2 if isinstance(x, (str, int, float))]
                                suggestions = [s for s in suggestions if s.strip()][:4]
                        except Exception:
                            suggestions = []
                if suggestions:
                    yield f"data: {json.dumps({'type': 'suggestions', 'data': suggestions}, ensure_ascii=False)}\n\n"
            except Exception:
                pass

            # å‘é€æœ€ç»ˆçš„ context ä¿¡æ¯
            context_info = usage_tracker.get_usage_info()
            yield f"data: {json.dumps({'type': 'context', 'data': context_info}, ensure_ascii=False)}\n\n"

            # å‘é€ç»“æŸäº‹ä»¶
            yield f"data: {json.dumps({'type': 'end', 'message': 'ç”Ÿæˆå®Œæˆ'}, ensure_ascii=False)}\n\n"
            
            # æ‰“å°ç»Ÿè®¡
            usage_tracker.log_summary()
            logger.info("âœ… æµå¼èŠå¤©è¯·æ±‚å¤„ç†å®Œæˆ")
            
        except Exception as e:
            error_msg = f"æµå¼å¤„ç†å‡ºé”™: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.exception(e)
            
            # å‘é€é”™è¯¯äº‹ä»¶
            error_data = {
                "type": "error",
                "message": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯",
                "error": str(e),
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
    
    # è¿”å› SSE å“åº”
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
        },
    )


@router.get("/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£
    
    Returns:
        å¥åº·çŠ¶æ€
    """
    return {
        "status": "healthy",
        "service": "chat",
        "version": settings.app_version,
    }


@router.get("/modes")
async def get_available_modes():
    """
    è·å–å¯ç”¨çš„ Agent æ¨¡å¼åˆ—è¡¨
    
    Returns:
        æ¨¡å¼åˆ—è¡¨åŠå…¶æè¿°
    """
    from core.prompts import SYSTEM_PROMPTS
    
    modes = {}
    for mode_name in SYSTEM_PROMPTS.keys():
        # æå–æ¯ä¸ªæ¨¡å¼çš„ç®€çŸ­æè¿°
        prompt = SYSTEM_PROMPTS[mode_name]
        first_line = prompt.split('\n')[0]
        modes[mode_name] = first_line
    
    return {
        "modes": modes,
        "default": "default",
    }
