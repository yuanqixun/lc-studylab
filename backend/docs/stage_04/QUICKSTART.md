# DeepAgent å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨ 5 åˆ†é’Ÿå†…å¼€å§‹ä½¿ç”¨ DeepAgent æ·±åº¦ç ”ç©¶åŠŸèƒ½ã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

1. **Python ç¯å¢ƒ**ï¼šPython 3.10+
2. **API Keys**ï¼š
   - OpenAI API Keyï¼ˆå¿…éœ€ï¼‰
   - Tavily API Keyï¼ˆå¯é€‰ï¼Œç”¨äºç½‘ç»œæœç´¢ï¼‰

## ğŸ”§ é…ç½®

### 1. è®¾ç½®ç¯å¢ƒå˜é‡

ç¼–è¾‘ `backend/.env` æ–‡ä»¶ï¼š

```bash
# å¿…éœ€
OPENAI_API_KEY=your_openai_api_key_here

# å¯é€‰ï¼ˆç”¨äºç½‘ç»œæœç´¢ï¼‰
TAVILY_API_KEY=your_tavily_api_key_here
```

### 2. éªŒè¯å®‰è£…

```bash
cd backend
python -c "from deep_research import create_deep_research_agent; print('âœ… DeepAgent å·²å®‰è£…')"
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šåŸºç¡€ç ”ç©¶ï¼ˆPython ä»£ç ï¼‰

```python
from deep_research import create_deep_research_agent

# 1. åˆ›å»º DeepAgent
agent = create_deep_research_agent(
    thread_id="my_first_research",
    enable_web_search=True,
    enable_doc_analysis=False,
)

# 2. æ‰§è¡Œç ”ç©¶
result = agent.research("LangChain 1.0 æœ‰å“ªäº›ä¸»è¦æ–°ç‰¹æ€§ï¼Ÿ")

# 3. æŸ¥çœ‹ç»“æœ
print("ç ”ç©¶çŠ¶æ€:", result["status"])
print("\næœ€ç»ˆæŠ¥å‘Š:")
print(result["final_report"])

# 4. æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
from core.tools.filesystem import get_filesystem
fs = get_filesystem("my_first_research")
files = fs.list_files()
print("\nç”Ÿæˆçš„æ–‡ä»¶:")
for f in files:
    print(f"  - {f}")
```

### ç¤ºä¾‹ 2ï¼šä½¿ç”¨ API

#### æ­¥éª¤ 1ï¼šå¯åŠ¨æœåŠ¡å™¨

```bash
cd backend
bash start_server.sh
```

#### æ­¥éª¤ 2ï¼šå¯åŠ¨ç ”ç©¶ä»»åŠ¡

```bash
curl -X POST "http://localhost:8000/deep-research/start" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "åˆ†æ AI é¢†åŸŸçš„æœ€æ–°è¶‹åŠ¿",
    "enable_web_search": true,
    "enable_doc_analysis": false
  }'
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "status": "success",
  "thread_id": "research_abc123",
  "message": "ç ”ç©¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°æ‰§è¡Œ",
  "estimated_time": "5-10åˆ†é’Ÿ"
}
```

#### æ­¥éª¤ 3ï¼šæŸ¥è¯¢çŠ¶æ€

```bash
# ä½¿ç”¨ä¸Šä¸€æ­¥è¿”å›çš„ thread_id
curl "http://localhost:8000/deep-research/status/research_abc123"
```

#### æ­¥éª¤ 4ï¼šè·å–ç»“æœ

```bash
# ç­‰å¾…ä»»åŠ¡å®Œæˆå
curl "http://localhost:8000/deep-research/result/research_abc123"
```

#### æ­¥éª¤ 5ï¼šæŸ¥çœ‹æ–‡ä»¶

```bash
# åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
curl "http://localhost:8000/deep-research/files/research_abc123"

# è¯»å–æœ€ç»ˆæŠ¥å‘Š
curl "http://localhost:8000/deep-research/file/research_abc123/reports/final_report.md"
```

### ç¤ºä¾‹ 3ï¼šå®Œæ•´ç ”ç©¶ï¼ˆå«æ–‡æ¡£åˆ†æï¼‰

```python
from deep_research import create_deep_research_agent
from rag import get_embeddings, load_vector_store, create_retriever_tool

# 1. åŠ è½½æ–‡æ¡£ç´¢å¼•
embeddings = get_embeddings()
vector_store = load_vector_store("data/indexes/test_index", embeddings)
retriever = vector_store.as_retriever()
retriever_tool = create_retriever_tool(retriever)

# 2. åˆ›å»º DeepAgentï¼ˆå¯ç”¨æ–‡æ¡£åˆ†æï¼‰
agent = create_deep_research_agent(
    thread_id="full_research_001",
    enable_web_search=True,
    enable_doc_analysis=True,
    retriever_tool=retriever_tool,
)

# 3. æ‰§è¡Œç ”ç©¶
result = agent.research("ä»€ä¹ˆæ˜¯ RAGï¼Ÿå®ƒæœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ")

# 4. æŸ¥çœ‹ç»“æœ
print(result["final_report"])
```

## ğŸ§ª è¿è¡Œæµ‹è¯•

### å¿«é€Ÿæµ‹è¯•

```bash
cd backend
python scripts/test_deep_research.py
```

### æµ‹è¯•è¾“å‡ºç¤ºä¾‹

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     DeepAgent æ·±åº¦ç ”ç©¶åŠŸèƒ½æµ‹è¯•å¥—ä»¶
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å½“å‰é…ç½®:
  OpenAI API: âœ… å·²é…ç½®
  Tavily API: âœ… å·²é…ç½®
  æ¨¡å‹: gpt-4o
  æ•°æ®ç›®å½•: data

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  æµ‹è¯• 1: æ–‡ä»¶ç³»ç»ŸåŠŸèƒ½                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

âœ… æ–‡ä»¶ç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚  æµ‹è¯• 2: åŸºç¡€ç ”ç©¶ï¼ˆç½‘ç»œæœç´¢ï¼‰             â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

âœ… åŸºç¡€ç ”ç©¶æµ‹è¯•é€šè¿‡ï¼

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æµ‹è¯•æ€»ç»“
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é€šè¿‡: 2 | å¤±è´¥: 0 | è·³è¿‡: 2

âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ğŸ‰
```

## ğŸ“ æ–‡ä»¶ç»“æ„

ç ”ç©¶å®Œæˆåï¼Œä¼šåœ¨ `data/research/{thread_id}/` ä¸‹ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
data/research/my_first_research/
â”œâ”€â”€ plans/
â”‚   â””â”€â”€ research_plan.md      # ç ”ç©¶è®¡åˆ’
â”œâ”€â”€ notes/
â”‚   â”œâ”€â”€ web_research.md       # ç½‘ç»œæœç´¢ç¬”è®°
â”‚   â””â”€â”€ doc_analysis.md       # æ–‡æ¡£åˆ†ææŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ final_report.md       # æœ€ç»ˆç ”ç©¶æŠ¥å‘Š
â””â”€â”€ temp/
    â””â”€â”€ ...                   # ä¸´æ—¶æ–‡ä»¶
```

## ğŸ¯ ç ”ç©¶é—®é¢˜ç¤ºä¾‹

### æŠ€æœ¯è°ƒç ”
```
"åˆ†æ LangChain 1.0 ç›¸æ¯” 0.x ç‰ˆæœ¬çš„ä¸»è¦æ”¹è¿›"
"å¯¹æ¯” FastAPI å’Œ Flask çš„æ€§èƒ½å’Œç‰¹æ€§"
"æ€»ç»“ Python å¼‚æ­¥ç¼–ç¨‹çš„æœ€ä½³å®è·µ"
```

### è¶‹åŠ¿åˆ†æ
```
"åˆ†æ 2024 å¹´ AI é¢†åŸŸçš„ä¸»è¦æŠ€æœ¯è¶‹åŠ¿"
"æ€»ç»“å¤§è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•"
"ç ”ç©¶è¾¹ç¼˜è®¡ç®—çš„åº”ç”¨åœºæ™¯å’ŒæŒ‘æˆ˜"
```

### å¯¹æ¯”ç ”ç©¶
```
"å¯¹æ¯” OpenAI GPT-4 å’Œ Anthropic Claude çš„èƒ½åŠ›å·®å¼‚"
"åˆ†æ PostgreSQL å’Œ MongoDB çš„é€‚ç”¨åœºæ™¯"
"å¯¹æ¯” React å’Œ Vue çš„å¼€å‘ä½“éªŒ"
```

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. è®¾è®¡å¥½çš„ç ”ç©¶é—®é¢˜

**å¥½çš„é—®é¢˜ç‰¹å¾ï¼š**
- å…·ä½“æ˜ç¡®
- æœ‰ç ”ç©¶ä»·å€¼
- èŒƒå›´é€‚ä¸­
- å¯ä»¥é‡åŒ–

**ç¤ºä¾‹ï¼š**
- âœ… "åˆ†æ LangChain 1.0 çš„ä¸‰ä¸ªä¸»è¦æ–°ç‰¹æ€§"
- âŒ "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ"

### 2. é€‰æ‹©åˆé€‚çš„æ¨¡å¼

| æ¨¡å¼ | é€‚ç”¨åœºæ™¯ | é¢„è®¡æ—¶é—´ |
|------|---------|---------|
| ä»…ç½‘ç»œæœç´¢ | å¿«é€Ÿäº†è§£ã€æœ€æ–°ä¿¡æ¯ | 3-5 åˆ†é’Ÿ |
| ç½‘ç»œ + æ–‡æ¡£ | æ·±å…¥åˆ†æã€ç»“åˆå†…éƒ¨çŸ¥è¯† | 5-10 åˆ†é’Ÿ |

### 3. ç®¡ç†ç ”ç©¶ä»»åŠ¡

```python
# ä½¿ç”¨æœ‰æ„ä¹‰çš„ thread_id
thread_id = f"research_{topic}_{date}"

# å®šæœŸæ¸…ç†
from core.tools.filesystem import get_filesystem
fs = get_filesystem(thread_id)
fs.cleanup()  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
```

### 4. è¯»å–ç ”ç©¶ç»“æœ

```python
# è¯»å–æœ€ç»ˆæŠ¥å‘Š
fs = get_filesystem("my_research")
report = fs.read_file("final_report.md", subdirectory="reports")

# æœç´¢ç‰¹å®šå†…å®¹
results = fs.search_files("å…³é”®è¯")
for result in results:
    print(f"{result['filename']}: {result['match_count']} ä¸ªåŒ¹é…")
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ç ”ç©¶ä»»åŠ¡å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A:** æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. API Key æ˜¯å¦æ­£ç¡®é…ç½®
2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼š`logs/app.log`

```bash
# æŸ¥çœ‹æœ€æ–°æ—¥å¿—
tail -f logs/app.log
```

### Q2: å¦‚ä½•æé«˜æŠ¥å‘Šè´¨é‡ï¼Ÿ

**A:** 
1. ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰
2. è®¾è®¡æ›´å…·ä½“çš„ç ”ç©¶é—®é¢˜
3. å¯ç”¨æ–‡æ¡£åˆ†æï¼Œç»“åˆå†…éƒ¨çŸ¥è¯†
4. è°ƒæ•´æœç´¢å…³é”®è¯

### Q3: ç ”ç©¶é€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

**A:**
1. ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4o-miniï¼‰
2. å‡å°‘æœç´¢æ¬¡æ•°
3. ç¦ç”¨æ–‡æ¡£åˆ†æï¼ˆå¦‚æœä¸éœ€è¦ï¼‰
4. ä½¿ç”¨åŸºç¡€ç ”ç©¶æ¨¡å¼

### Q4: å¦‚ä½•æŸ¥çœ‹ä¸­é—´ç»“æœï¼Ÿ

**A:**
```python
from core.tools.filesystem import get_filesystem

fs = get_filesystem("your_thread_id")

# åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
files = fs.list_files()
print(files)

# è¯»å–ç ”ç©¶ç¬”è®°
notes = fs.read_file("web_research.md", subdirectory="notes")
print(notes)
```

## ğŸ“š ä¸‹ä¸€æ­¥

- ğŸ“– é˜…è¯» [å®Œæ•´æ–‡æ¡£](./README.md)
- ğŸ”§ æŸ¥çœ‹ [API æ–‡æ¡£](./API.md)
- ğŸ’» æŸ¥çœ‹ [å®æ–½è®¡åˆ’](./STAGE4_PLAN.md)
- ğŸ“ å­¦ä¹  [æœ€ä½³å®è·µ](./README.md#æœ€ä½³å®è·µ)

## ğŸ‰ å®Œæˆï¼

æ­å–œï¼ä½ å·²ç»å­¦ä¼šäº† DeepAgent çš„åŸºæœ¬ä½¿ç”¨ã€‚

ç°åœ¨ä½ å¯ä»¥ï¼š
- âœ… åˆ›å»ºæ·±åº¦ç ”ç©¶ä»»åŠ¡
- âœ… ä½¿ç”¨ API æ¥å£
- âœ… ç®¡ç†ç ”ç©¶æ–‡ä»¶
- âœ… è¯»å–ç ”ç©¶ç»“æœ

å¼€å§‹ä½ çš„ç¬¬ä¸€ä¸ªç ”ç©¶å§ï¼ğŸš€

---

**éœ€è¦å¸®åŠ©ï¼Ÿ**
- æŸ¥çœ‹ [README.md](./README.md)
- æŸ¥çœ‹æ—¥å¿—ï¼š`logs/app.log`
- è¿è¡Œæµ‹è¯•ï¼š`python scripts/test_deep_research.py`

